{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI_MVOulYplG"
   },
   "source": [
    "# Algonauts + Net2Brain CCN23 Hackathon\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "1. **Loading Models**: Load _Scene Classification_ and _Scene Parsing_ Artificial Neural Networks (ANNs) using `Net2Brain`.\n",
    "2. **Extracting features from ANNs internal representations**: Extract model features from  these ANNs processing a subset of the `Algonauts Challenge` Dataset.\n",
    "3. **Training encoding Models**: Build encoding models that predict brain data from the DNN features using `Net2Brain`.\n",
    "4. **Plotting**: Visualize the results on ROIs.\n",
    "5. **Other Models**: Try other DNNs available through `Net2Brain`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhXmDhmt6Gd8"
   },
   "source": [
    "## If working on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install Net2Brain and relevant dependencies"
   ],
   "metadata": {
    "id": "QhpoTT0AwbpJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPQh1MlxCtCT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "18b934d5-d508-4af4-ee11-050ae7f612fc"
   },
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/cvai-roig-lab/Net2Brain"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtrlzxH1sbn-"
   },
   "source": [
    "### Restart runtime and install _nilearn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hx2M2yAFTawt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1e11422e-8049-436c-ec56-073554ea3a2d"
   },
   "outputs": [],
   "source": [
    "!pip install nilearn==0.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSJLyswUSndh"
   },
   "source": [
    "### Mount the workshop data on your drive runtime\n",
    "Before running the tutorial code you need to select [this](https://t.ly/jkIu-) folder and go to `organize` and `add shortcut`. You will then need to create a shortcut (without copying or taking space) of the folder to a desired path in your Google Drive, from which you can read the content after mounting using `drive.mount()`.\n",
    "\n",
    "Please don't forget to edit the `data_dir` variable below with the path on your Drive to this shortcut folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEEJ6KpYSqSK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fa050793-624d-4610-88ff-2b5b40ea95bf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "data_dir = '/content/drive/MyDrive/put_data_here' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS4tXYtaYplL"
   },
   "source": [
    "## If working locally"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are running this tutorial locally you need to make sure you have downloaded the challenge data located in folder `\\subj01`. You can download it from [here]((https://t.ly/jkIu-).\n",
    "\n",
    "Then uncomment and edit the `data_dir` variable below with the path to the parent folder containing the `\\subj01` data folder."
   ],
   "metadata": {
    "id": "61uyWEjV0ipC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir = '.'"
   ],
   "metadata": {
    "id": "Rzoi_PFG047P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN8Z2MYMTiFZ"
   },
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCne9nCSTSk_"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from nilearn import datasets, plotting"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_device = torch.device(device)"
   ],
   "metadata": {
    "id": "MisvoQfW0G6l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About Net2Brain"
   ],
   "metadata": {
    "id": "Sf-aoA2b1qBb"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn_2AvR-5ah7"
   },
   "source": [
    "__Net2Brain__ allows you to use one of over 1000 Deep Neural Networks (DNNs) for your experiments comparing human brain activity with the activations of artificial neural networks. The DNNs in __Net2Brain__ are obtained from what we call different _netsets_, which are libraries that provide different pretrained models.\n",
    "\n",
    "__Net2Brain__ provides access to the following _netsets_:\n",
    "- [Standard torchvision](https://pytorch.org/vision/stable/models.html) (`standard`).\n",
    "This netset is a collection of the torchvision models including models for image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n",
    "- [Timm](https://github.com/rwightman/pytorch-image-models#models) (`timm`).\n",
    "A deep-learning library created by Ross Wightman that contains a collection of state-of-the-art computer vision models.\n",
    "- [PyTorch Hub](https://pytorch.org/docs/stable/hub.html) (`pytorch`).\n",
    "These models are accessible through the torch.hub API and are trained for different visual tasks. They are not included in the torchvision module.\n",
    "- [Unet](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/) (`unet`).\n",
    "Unet also is available through the torch.hub.API and is trained for abnormality segmentation in brain MRI.\n",
    "- [Taskonomy](https://github.com/StanfordVL/taskonomy) (`taskonomy`). A set of networks trained for different visual tasks, like Keypoint-Detection, Depth-Estimation, Reshading, etc. The initial idea for these networks was to find relationships between different visual tasks.\n",
    "- [Slowfast](https://github.com/facebookresearch/pytorchvideo) (`pyvideo`).\n",
    "These models are state-of-the-art video classification models trained on the Kinetics 400 dataset, acessible through the torch.hub API.\n",
    "- [CLIP](https://github.com/openai/CLIP) (`clip`).\n",
    "CLIP (Contrastive Language-Image Pre-Training) is a vision+language multimodal neural network trained on a variety of (image, text) pairs.\n",
    "- [CorNet](https://github.com/dicarlolab/CORnet) (`cornet`).\n",
    "A set of neural networks whose structure is supposed to resemble the one of the ventral visual pathway and therefore implements more recurrent connections that are commonplace in the VVS.\n",
    "- [Detectron2](https://github.com/facebookresearch/Detectron) (`detectron2`).\n",
    "Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It covers models trained for object classification and detection such as instance, panoptic and keypoint detection.\n",
    "- [VISSL](https://github.com/facebookresearch/vissl) (`vissl`).\n",
    "VISSL provides reference implementation of a large number of self-supervision approaches.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**Net2Brain** has three main components:\n",
    "1. **ANN Selection**\n",
    "> Select models by specific architectures, training objectives, or pre-training data\n",
    "2. **Feature Extraction**\n",
    "> Expects images or videos in .jpg, .png, or .mp4 format\n",
    "3. **Brain-ANN comparison**\n",
    "> Using forward encoding or RSA [Subjects x ROIs x Stimuli Condition x Stimuli Condition]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unraveling brain functionality with ANNs\n",
    "\n",
    "Research correlating brain and artificial neural network representations can be used to unravel brain functionality across both space and time.\n",
    "\n",
    "In this hackathon we will explore how different brain areas (ROIs) correlate distinctively with ANNs trained on different visual tasks.\n"
   ],
   "metadata": {
    "id": "WespjI4H4oVt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This exploration is inspired by the study of [Dwivedi et al. (2021)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009267), where they used the __Taskonomy__ netset, that contains ANNs trained in the following visual tasks:"
   ],
   "metadata": {
    "id": "GEP20L9I4d_q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display(Image.open(Path(data_dir) / 'subj01/misc' / 'task_similarity_tree.png'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "7nm-vj5j8aGy",
    "outputId": "86ec37da-982b-4ecf-84bb-59bfe0aba076"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This study grouped the Taskonomy ANNs into three groups:\n",
    "\n",
    "- __2D Tasks__: segment_unsup2d, inpainting, keypoints2d, jigsaw, autoencoding, denoising.\n",
    "\n",
    "- __3D Tasks__: reshading , curvature, depth_euclidean, keypoints3d, normal.\n",
    "\n",
    "- __Semantic Tasks__: class_object, class_scene, segment_semantic.\n",
    "\n",
    "And they found the representations of each of these groups correlated more strongly with different visual ROIs:"
   ],
   "metadata": {
    "id": "kJDiL48E-kmN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display(Image.open(Path(data_dir) / 'subj01/misc' / 'unravelingBrainFunc.png'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "RXu-N1Tl-pkp",
    "outputId": "73ad2f16-cfa2-4007-b0b3-5b7bf145d0fc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu4sYz1tLpRL"
   },
   "source": [
    "## Step 0: Load the _Algonauts Challenge_ dataset\n",
    "\n",
    "For this hackathon, we will use a sub-selection of the [Algonauts 2023](http://algonauts.csail.mit.edu/challenge.html) challenge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz5YVvM8Uyce"
   },
   "outputs": [],
   "source": [
    "subj = '01' # We will only use subject 01 for this hackathon\n",
    "\n",
    "sml_stim = Path(data_dir) / f'subj{subj}' / 'sml_images'\n",
    "sml_fmri = Path(data_dir) / f'subj{subj}' / 'sml_fmri'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muHVfP7ze4Ke",
    "outputId": "4b4f7413-f1bc-4749-8e5e-9f14114a02fd"
   },
   "outputs": [],
   "source": [
    "# Create sorted lists of image file names\n",
    "sml_img_list = os.listdir(sml_stim)\n",
    "sml_img_list.sort()\n",
    "print(f'Total images: {len(sml_img_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoD_SmGWUxX_"
   },
   "source": [
    "## Step 1: Selecting a pretrained ANN with Net2Brain\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13PxRKwPYplQ"
   },
   "source": [
    "Pick one of the following three models by hackathon group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3fGxbu3YplQ"
   },
   "outputs": [],
   "source": [
    "# model_name = 'segment_unsup2d' # For 2D features\n",
    "model_name = 'reshading'       # For 3D features\n",
    "# model_name = 'class_object'    # For Semantic features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To extract activations from a pretrained model of one of the netsets available with Net2Brain, you must first initialize the `FeatureExtractor` class and specify the name of the model as well as the netset it belongs to.\n",
    "\n",
    "__Note__: If you want to implement your _own_ pre-trained ANN, use the model as an argument of `Feature Extractor` like:\n",
    "`FeatureExtractor(model=my_model, device='cuda')`"
   ],
   "metadata": {
    "id": "-1NKMZI-ClWK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from net2brain.feature_extraction import FeatureExtractor\n",
    "\n",
    "fx_model = FeatureExtractor(model=model_name,\n",
    "                            netset='Taskonomy',\n",
    "                            device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "ff1ee71b670748bdad434974c1f35ed3",
      "9cc2b97dec164596bc1c81ea35cf32db",
      "192e4da6c43b4086829412490c57ffaf",
      "e756325c6a9e4ce59a5002756e0799fe",
      "ceb76677e85a48fbbdb268b491c70a34",
      "4b9b7372b56c41c5ad8df47f24e1dcc1",
      "bca2e8f73f70430bb38a85128e383b2f",
      "3e202435bb1c4739a62c868de5440b6b",
      "6295ff2bdf4b4a1fa0b5ec6b2c873ef6",
      "4e8aa56a055a469689e1346e586245ad",
      "2ef2800ce8584cdcbf9957edbb228b65"
     ]
    },
    "id": "u1bic3YNCpcm",
    "outputId": "f480ff8e-7620-43d9-9567-ad60d6f68065"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP-p77eFH7Mf"
   },
   "source": [
    "Note that by default Net2Brain selects which layers of the model are going to be used to extract the features from.\n",
    "To view the layers that are set to be extracted, you can inspect the attribute `layers_to_extract`, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhXqO0Bx1qqe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5feeb5bf-e55d-43d2-a1d9-659b31f7980e"
   },
   "outputs": [],
   "source": [
    "fx_model.layers_to_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also specify which layers you want to extract from the model.\n",
    "\n",
    "To view a complete list of all available layers, you can use the class method `get_all_layers()` and overwrite the `layers_to_extract` argument with your desired subset."
   ],
   "metadata": {
    "id": "363anVPmDagF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fx_model.get_all_layers()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjLwS7gSDZRn",
    "outputId": "01154de5-4b65-41a0-f9d0-df6981a43bbb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssVK6gFQUi-H"
   },
   "source": [
    "## Step 2: Extracting interal representations from the ANN\n",
    "To extract the activations from the specified layers, you can use the `extract()` method and provide the path to the images that you want to run through the network.\n",
    "You can choose to save the features in different formats (numpy arrays, pytorch tensors, or into the `dataset` class from the [rsa](https://rsatoolbox.readthedocs.io/en/stable/) toolbox).\n",
    "\n",
    "For this tutorial, we will use the numpy array (`npz`) format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9c_ZfpF_Jea",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e8ebf738-58d2-44a5-d85c-677999e051d4"
   },
   "outputs": [],
   "source": [
    "# Create features for the chosen model\n",
    "ft_path = f'sml_feats_{model_name}'\n",
    "fx_model.extract(data_path=sml_stim,\n",
    "                 save_path=ft_path, consolidate_per_layer=False,\n",
    "                 layers_to_extract=['layer4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13O7jnohYple"
   },
   "source": [
    "## Step 3: Building Encoding models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "To find how well the ANN's features predict brain activity, we will train an encoding model using a 3-fold cross-validation process.\n",
    "\n",
    "For every fold, we will:\n",
    "1. Split the data into training/validation sets.\n",
    "2. Use PCA to reduce the feature dimentions.\n",
    "3. Train a regression model predicting voxel activations from ANN's representations.\n",
    "\n",
    "We can train this model using the function the class `encoding` from Net2Brain.\n",
    "The training will take some minutes."
   ],
   "metadata": {
    "id": "Mudzn3v_Fhiz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUkbuaOHYplf",
    "outputId": "47149981-da7f-4bbc-a61b-c81b0c1a6fc4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "import net2brain.evaluations.encoding as encoding\n",
    "\n",
    "roi_path = str(sml_fmri)\n",
    "model_brain_df, model_brain_corr = encoding.linear_encoding(ft_path, roi_path,\n",
    "                                                            model_name,\n",
    "                                                            trn_tst_split=0.8,\n",
    "                                                            n_folds=3,\n",
    "                                                            n_components=70,\n",
    "                                                            batch_size=300,\n",
    "                                                            return_correlations=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_brain_df[['ROI', 'R']]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "wZus5R90LH-6",
    "outputId": "bfb9dd22-fc4d-48e3-c5a9-7de2acdbdc82"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr45GX51Yplf"
   },
   "outputs": [],
   "source": [
    "## Save correlation values\n",
    "# np.save(f'model_name_corr.npy', model_brain_corr)\n",
    "\n",
    "## Save dataframe\n",
    "model_brain_df.to_json(f'{model_name}_df.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPsurvRWYpln"
   },
   "source": [
    "## Step 4: Plot correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will plot the ANN-Brain correlation results using Net2Brain and _nilearn_."
   ],
   "metadata": {
    "id": "kr1PUanRGffk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duwYWN6WYplo"
   },
   "outputs": [],
   "source": [
    "d3_df = pd.read_json(Path(data_dir) / f'subj{subj}' / 'reshading_df.json')\n",
    "sm_df = pd.read_json(Path(data_dir) / f'subj{subj}' / 'class_object_df.json')\n",
    "d2_df = pd.read_json(Path(data_dir) / f'subj{subj}' / 'segment_unsup2d_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_brain_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rxe0CGFzYplo"
   },
   "outputs": [],
   "source": [
    "from net2brain.evaluations.plotting import Plotting\n",
    "\n",
    "# Plotting with significance\n",
    "plotter = Plotting([d2_df, d3_df, sm_df])\n",
    "# plotter = Plotting([model_brain_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBMYvmGwYplo",
    "outputId": "d9df7967-e069-4520-adee-70596cac4bbf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    }
   },
   "outputs": [],
   "source": [
    "results_df = plotter.plot(metric=\"R\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#results_df = plotter.plot(metric=\"R\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "5K5nA51CYwWr",
    "outputId": "48a97064-3d1a-40d6-9506-64ca5c2e1453"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXwdx5UtYplo"
   },
   "source": [
    "### Visualize the correlations using _nilearn_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqgKxzAWYplo"
   },
   "source": [
    "We will load the ROI indices extracted as seen in the developer-kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY1rXWfdYplo"
   },
   "outputs": [],
   "source": [
    "roi_idx = np.load((Path(data_dir) / f'subj{subj}' / 'sml_roi_idx_map.npy'), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrInT-VsYplo"
   },
   "source": [
    "And we will load the subject to fsaverage projection, resulting in 163842 brain vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NW0cKk4Yplo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ab4b0c60-70cb-4ee1-ee8e-5ef554106306"
   },
   "outputs": [],
   "source": [
    "fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\n",
    "\n",
    "masks_dir = Path(data_dir) / f'subj{subj}' / 'roi_masks'\n",
    "rh_fsaverage = np.load((masks_dir / 'rh.all-vertices_fsaverage_space.npy'), allow_pickle=True)\n",
    "print(f'Shape of fsaverage: {rh_fsaverage.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NCWKXXyYplp"
   },
   "outputs": [],
   "source": [
    "fs_idx = np.where(rh_fsaverage)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qei5t-OjYplp"
   },
   "outputs": [],
   "source": [
    "model_brain_dict = dict(zip(model_brain_df.ROI, model_brain_df.R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toi31M3wYplp"
   },
   "outputs": [],
   "source": [
    "plot_data = np.zeros(rh_fsaverage.shape)\n",
    "plot_data[fs_idx[roi_idx['V1']]]  = np.ones(fs_idx[roi_idx['V1']].shape)* model_brain_dict['rh_V1_fmri']\n",
    "plot_data[fs_idx[roi_idx['V2']]]  = np.ones(fs_idx[roi_idx['V2']].shape)* model_brain_dict['rh_V2_fmri']\n",
    "plot_data[fs_idx[roi_idx['V3']]]  = np.ones(fs_idx[roi_idx['V3']].shape)* model_brain_dict['rh_V3_fmri']\n",
    "plot_data[fs_idx[roi_idx['PPA']]] = np.ones(fs_idx[roi_idx['PPA']].shape) * model_brain_dict['rh_PPA_fmri']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuksSRtFYplp"
   },
   "source": [
    "You can use `flat_right` and `curv_right` to vizualize a flatmap instead of cortical view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzUGceJWYplp"
   },
   "outputs": [],
   "source": [
    "view = plotting.view_surf(\n",
    "    surf_mesh=fsaverage['infl_right'],\n",
    "    surf_map=plot_data, bg_map=fsaverage['sulc_right'],\n",
    "    threshold=1e-14, colorbar=True, symmetric_cmap= False,\n",
    "    cmap=plt.get_cmap('twilight_shifted')\n",
    ")\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iewOAJHTYplp"
   },
   "source": [
    "### Which taskonomy group was the most correlated with different ROIs?\n",
    "<b style='color:#0000ff;'>2D Tasks </b>, <b style='color:#00ff00;'>3D Tasks </b>, or   <b style='color:#ff00ff;'>Semantic Tasks </b>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQ1ZWkFhYplp"
   },
   "outputs": [],
   "source": [
    "taskonomy_cmap = plc.LinearSegmentedColormap.from_list(\"\", [\"#0000ff\",\"#00ff00\",\"#ff00ff\"])\n",
    "taskonomy_corr_dict =  {roi : [d2_df.R[ii],d3_df.R[ii],sm_df.R[ii]] for ii,roi in enumerate(d2_df.ROI) }\n",
    "plot_data = np.zeros(rh_fsaverage.shape)\n",
    "for roi in roi_idx.keys():\n",
    "    plot_data[fs_idx[roi_idx[roi]]]  = np.ones(fs_idx[roi_idx[roi]].shape) * \\\n",
    "    (np.argmax(taskonomy_corr_dict['rh_'+roi+'_fmri'])+1e-10)\n",
    "view = plotting.view_surf(fsaverage.infl_right, plot_data, bg_map=fsaverage.sulc_right, threshold=1e-14,\n",
    "                          cmap=taskonomy_cmap, colorbar=False, symmetric_cmap=False)\n",
    "view"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cS4tXYtaYplL"
   ],
   "gpuClass": "premium",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "ff1ee71b670748bdad434974c1f35ed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cc2b97dec164596bc1c81ea35cf32db",
       "IPY_MODEL_192e4da6c43b4086829412490c57ffaf",
       "IPY_MODEL_e756325c6a9e4ce59a5002756e0799fe"
      ],
      "layout": "IPY_MODEL_ceb76677e85a48fbbdb268b491c70a34"
     }
    },
    "9cc2b97dec164596bc1c81ea35cf32db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b9b7372b56c41c5ad8df47f24e1dcc1",
      "placeholder": "​",
      "style": "IPY_MODEL_bca2e8f73f70430bb38a85128e383b2f",
      "value": "100%"
     }
    },
    "192e4da6c43b4086829412490c57ffaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e202435bb1c4739a62c868de5440b6b",
      "max": 94874754,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6295ff2bdf4b4a1fa0b5ec6b2c873ef6",
      "value": 94874754
     }
    },
    "e756325c6a9e4ce59a5002756e0799fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e8aa56a055a469689e1346e586245ad",
      "placeholder": "​",
      "style": "IPY_MODEL_2ef2800ce8584cdcbf9957edbb228b65",
      "value": " 90.5M/90.5M [00:00&lt;00:00, 200MB/s]"
     }
    },
    "ceb76677e85a48fbbdb268b491c70a34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b9b7372b56c41c5ad8df47f24e1dcc1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bca2e8f73f70430bb38a85128e383b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e202435bb1c4739a62c868de5440b6b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6295ff2bdf4b4a1fa0b5ec6b2c873ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e8aa56a055a469689e1346e586245ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ef2800ce8584cdcbf9957edbb228b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
