{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXmDhmt6Gd8"
      },
      "source": [
        "# Installing Net2Brain and Relevant Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vPQh1MlxCtCT"
      },
      "outputs": [],
      "source": [
        "# !pip install -U git+https://github.com/cvai-roig-lab/Net2Brain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtrlzxH1sbn-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Restart Runtime\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWzawRYH6oQ_"
      },
      "source": [
        "# Net2Brain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn_2AvR-5ah7"
      },
      "source": [
        "__Net2Brain__ allows you to use one of over 600 Deep Neural Networks (DNNs) for your experiments comparing human brain activity with the activations of artificial neural networks. The DNNs in __Net2Brain__ are obtained from what we call different _netsets_, which are libraries that provide different pretrained models. \n",
        "\n",
        "__Net2Brain__ provides access to the following _netsets_:\n",
        "- [Standard torchvision](https://pytorch.org/vision/stable/models.html) (`standard`).\n",
        "This netset is a collection of the torchvision models including models for image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n",
        "- [Timm](https://github.com/rwightman/pytorch-image-models#models) (`timm`). \n",
        "A deep-learning library created by Ross Wightman that contains a collection of state-of-the-art computer vision models.\n",
        "- [PyTorch Hub](https://pytorch.org/docs/stable/hub.html) (`pytorch`). \n",
        "These models are accessible through the torch.hub API and are trained for different visual tasks. They are not included in the torchvision module.\n",
        "- [Unet](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/) (`unet`). \n",
        "Unet also is available through the torch.hub.API and is trained for abnormality segmentation in brain MRI.\n",
        "- [Taskonomy](https://github.com/StanfordVL/taskonomy) (`taskonomy`). A set of networks trained for different visual tasks, like Keypoint-Detection, Depth-Estimation, Reshading, etc. The initial idea for these networks was to find relationships between different visual tasks.\n",
        "- [Slowfast](https://github.com/facebookresearch/pytorchvideo) (`pyvideo`). \n",
        "These models are state-of-the-art video classification models trained on the Kinetics 400 dataset, acessible through the torch.hub API.\n",
        "- [CLIP](https://github.com/openai/CLIP) (`clip`). \n",
        "CLIP (Contrastive Language-Image Pre-Training) is a vision+language multimodal neural network trained on a variety of (image, text) pairs.\n",
        "- [CorNet](https://github.com/dicarlolab/CORnet) (`cornet`). \n",
        "A set of neural networks whose structure is supposed to resemble the one of the ventral visual pathway and therefore implements more recurrent connections that are commonplace in the VVS.\n",
        "- [Detectron2](https://github.com/facebookresearch/Detectron) (`detectron2`). \n",
        "Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It covers models trained for object classification and detection such as instance, panoptic and keypoint detection.\n",
        "- [VISSL](https://github.com/facebookresearch/vissl) (`vissl`). \n",
        "VISSL provides reference implementation of a large number of self-supervision approaches.\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Net2Brain** consists of 3 main parts:\n",
        "1. **Feature Extraction**\n",
        "  > Expects images or videos in .jpg, .png, or .mp4 format\n",
        "2. **Representational Dissimilarity Matrix (RDM) Creation**\n",
        "> Expects .npy-Files from our feature extractor\n",
        "3. **Representational Similarity Analysis (RSA)**\n",
        "> Expects RDMs in the format of [Subjects x ROIs x Stimuli Condition x Stimuli Condition] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqQdckZHWVR4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbm36FteFXql"
      },
      "source": [
        "\n",
        "# Step 0: Exploring the Toolbox - Model Taxonomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mhM7X2FXF2AI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vissl models are not installed\n",
            "detectron2 is not installed.\n"
          ]
        }
      ],
      "source": [
        "from net2brain.feature_extraction import show_all_architectures\n",
        "from net2brain.feature_extraction import show_all_netsets\n",
        "from net2brain.feature_extraction import show_taxonomy\n",
        "from net2brain.feature_extraction import print_netset_models\n",
        "\n",
        "from net2brain.feature_extraction import find_model_like_name\n",
        "from net2brain.feature_extraction import find_model_by_dataset\n",
        "from net2brain.feature_extraction import find_model_by_training_method\n",
        "from net2brain.feature_extraction import find_model_by_visual_task\n",
        "from net2brain.feature_extraction import find_model_by_custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TKygwloFdk0"
      },
      "source": [
        "To view a list of all available models along with the information on which netset they belong to, you can use the `print_all_models()` function to print them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NCgzKdvJyzMQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "NetSet: standard\n",
            "Models: ['AlexNet', 'ResNet18', 'ResNet34', 'ResNet50', 'ResNet101', 'ResNet152', 'Squeezenet1_0', 'Squeezenet1_1', 'VGG11', 'VGG11_bn', 'VGG13', 'VGG13_bn', 'VGG16', 'VGG16_bn', 'VGG19', 'VGG19_bn', 'Densenet121', 'Densenet161', 'Densenet169', 'Densenet201', 'GoogleNet', 'ShuffleNetV2x05', 'ShuffleNetV2x10', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet101_2', 'wide_resnet50_2', 'mnasnet05', 'mnasnet10', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_1_6gf', 'regnet_y_3_2gf', 'regnet_y_8gf', 'regnet_y_16gf', 'regnet_y_32gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_1_6gf', 'regnet_x_3_2gf', 'regnet_x_8gf', 'regnet_x_16gf', 'regnet_x_32gf']\n",
            "\n",
            "\n",
            "NetSet: toolbox\n",
            "Models: ['Places365', 'SceneParsing']\n",
            "\n",
            "\n",
            "NetSet: timm\n",
            "Models: ['adv_inception_v3', 'cait_m36_384', 'cait_m48_448', 'cait_s24_224', 'cait_s24_384', 'cait_s36_384', 'cait_xs24_384', 'cait_xxs24_224', 'cait_xxs24_384', 'cait_xxs36_224', 'cait_xxs36_384', 'coat_lite_mini', 'coat_lite_small', 'coat_lite_tiny', 'coat_mini', 'coat_tiny', 'convit_base', 'convit_small', 'convit_tiny', 'cspdarknet53', 'cspresnet50', 'cspresnext50', 'deit_base_distilled_patch16_224', 'deit_base_distilled_patch16_384', 'deit_base_patch16_224', 'deit_base_patch16_384', 'deit_small_distilled_patch16_224', 'deit_small_patch16_224', 'deit_tiny_distilled_patch16_224', 'deit_tiny_patch16_224', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'densenetblur121d', 'dla34', 'dla46_c', 'dla46x_c', 'dla60', 'dla60_res2net', 'dla60_res2next', 'dla60x', 'dla60x_c', 'dla102', 'dla102x', 'dla102x2', 'dla169', 'dm_nfnet_f0', 'dm_nfnet_f1', 'dm_nfnet_f2', 'dm_nfnet_f3', 'dm_nfnet_f4', 'dm_nfnet_f5', 'dm_nfnet_f6', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'eca_nfnet_l0', 'eca_nfnet_l1', 'eca_nfnet_l2', 'ecaresnet26t', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet269d', 'ecaresnetlight', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b1_pruned', 'efficientnet_b2', 'efficientnet_b2_pruned', 'efficientnet_b3', 'efficientnet_b3_pruned', 'efficientnet_b4', 'efficientnet_el', 'efficientnet_el_pruned', 'efficientnet_em', 'efficientnet_es', 'efficientnet_es_pruned', 'efficientnet_lite0', 'efficientnetv2_rw_m', 'efficientnetv2_rw_s', 'ens_adv_inception_resnet_v2', 'ese_vovnet19b_dw', 'ese_vovnet39b', 'fbnetc_100', 'gernet_l', 'gernet_m', 'gernet_s', 'ghostnet_100', 'gluon_inception_v3', 'gluon_resnet18_v1b', 'gluon_resnet34_v1b', 'gluon_resnet50_v1b', 'gluon_resnet50_v1c', 'gluon_resnet50_v1d', 'gluon_resnet50_v1s', 'gluon_resnet101_v1b', 'gluon_resnet101_v1c', 'gluon_resnet101_v1d', 'gluon_resnet101_v1s', 'gluon_resnet152_v1b', 'gluon_resnet152_v1c', 'gluon_resnet152_v1d', 'gluon_resnet152_v1s', 'gluon_resnext50_32x4d', 'gluon_resnext101_32x4d', 'gluon_resnext101_64x4d', 'gluon_senet154', 'gluon_seresnext50_32x4d', 'gluon_seresnext101_32x4d', 'gluon_seresnext101_64x4d', 'gluon_xception65', 'gmixer_24_224', 'gmlp_s16_224', 'hardcorenas_a', 'hardcorenas_b', 'hardcorenas_c', 'hardcorenas_d', 'hardcorenas_e', 'hardcorenas_f', 'hrnet_w18', 'hrnet_w18_small', 'hrnet_w18_small_v2', 'hrnet_w30', 'hrnet_w32', 'hrnet_w40', 'hrnet_w44', 'hrnet_w48', 'hrnet_w64', 'ig_resnext101_32x8d', 'ig_resnext101_32x16d', 'ig_resnext101_32x32d', 'ig_resnext101_32x48d', 'inception_resnet_v2', 'inception_v3', 'inception_v4', 'legacy_senet154', 'legacy_seresnet18', 'legacy_seresnet34', 'legacy_seresnet50', 'legacy_seresnet101', 'legacy_seresnet152', 'legacy_seresnext26_32x4d', 'legacy_seresnext50_32x4d', 'legacy_seresnext101_32x4d', 'levit_384', 'mixer_b16_224', 'mixer_b16_224_in21k', 'mixer_b16_224_miil', 'mixer_b16_224_miil_in21k', 'mixer_l16_224', 'mixer_l16_224_in21k', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_100', 'mobilenetv2_100', 'mobilenetv2_110d', 'mobilenetv2_120d', 'mobilenetv2_140', 'mobilenetv3_large_100', 'mobilenetv3_large_100_miil', 'mobilenetv3_large_100_miil_in21k', 'mobilenetv3_rw', 'nasnetalarge', 'nf_regnet_b1', 'nf_resnet50', 'nfnet_l0', 'pit_b_224', 'pit_b_distilled_224', 'pit_s_224', 'pit_s_distilled_224', 'pit_ti_224', 'pit_ti_distilled_224', 'pit_xs_224', 'pit_xs_distilled_224', 'pnasnet5large', 'regnetx_002', 'regnetx_004', 'regnetx_006', 'regnetx_008', 'regnetx_016', 'regnetx_032', 'regnetx_040', 'regnetx_064', 'regnetx_080', 'regnetx_120', 'regnetx_160', 'regnetx_320', 'regnety_002', 'regnety_004', 'regnety_006', 'regnety_008', 'regnety_016', 'regnety_032', 'regnety_040', 'regnety_064', 'regnety_080', 'regnety_120', 'regnety_160', 'regnety_320', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'res2net50_14w_8s', 'res2net50_26w_4s', 'res2net50_26w_6s', 'res2net50_26w_8s', 'res2net50_48w_2s', 'res2net101_26w_4s', 'res2next50', 'resmlp_12_224', 'resmlp_12_distilled_224', 'resmlp_24_224', 'resmlp_24_distilled_224', 'resmlp_36_224', 'resmlp_36_distilled_224', 'resmlp_big_24_224', 'resmlp_big_24_224_in22ft1k', 'resmlp_big_24_distilled_224', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet34', 'resnet34d', 'resnet50', 'resnet50d', 'resnet51q', 'resnet101d', 'resnet152d', 'resnet200d', 'resnetblur50', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50x1_bit_distilled', 'resnetv2_50x1_bitm', 'resnetv2_50x1_bitm_in21k', 'resnetv2_50x3_bitm', 'resnetv2_50x3_bitm_in21k', 'resnetv2_101x1_bitm', 'resnetv2_101x1_bitm_in21k', 'resnetv2_101x3_bitm', 'resnetv2_101x3_bitm_in21k', 'resnetv2_152x2_bit_teacher', 'resnetv2_152x2_bit_teacher_384', 'resnetv2_152x2_bitm', 'resnetv2_152x2_bitm_in21k', 'resnetv2_152x4_bitm', 'resnetv2_152x4_bitm_in21k', 'resnext50_32x4d', 'resnext50d_32x4d', 'resnext101_32x8d', 'selecsls42b', 'selecsls60', 'selecsls60b', 'semnasnet_100', 'seresnet50', 'seresnet152d', 'seresnext26d_32x4d', 'seresnext26t_32x4d', 'seresnext50_32x4d', 'spnasnet_100', 'ssl_resnet18', 'ssl_resnet50', 'ssl_resnext50_32x4d', 'ssl_resnext101_32x4d', 'ssl_resnext101_32x8d', 'ssl_resnext101_32x16d', 'swin_base_patch4_window7_224', 'swin_base_patch4_window7_224_in22k', 'swin_base_patch4_window12_384', 'swin_base_patch4_window12_384_in22k', 'swin_large_patch4_window7_224', 'swin_large_patch4_window7_224_in22k', 'swin_large_patch4_window12_384', 'swin_large_patch4_window12_384_in22k', 'swin_small_patch4_window7_224', 'swin_tiny_patch4_window7_224', 'swsl_resnet18', 'swsl_resnet50', 'swsl_resnext50_32x4d', 'swsl_resnext101_32x4d', 'swsl_resnext101_32x8d', 'swsl_resnext101_32x16d', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_efficientnetv2_b0', 'tf_efficientnetv2_b1', 'tf_efficientnetv2_b2', 'tf_efficientnetv2_b3', 'tf_efficientnetv2_l', 'tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21k', 'tf_efficientnetv2_m', 'tf_efficientnetv2_m_in21ft1k', 'tf_efficientnetv2_m_in21k', 'tf_efficientnetv2_s', 'tf_efficientnetv2_s_in21ft1k', 'tf_efficientnetv2_s_in21k', 'tf_inception_v3', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100', 'tnt_s_patch16_224', 'tv_densenet121', 'tv_resnet34', 'tv_resnet50', 'tv_resnet101', 'tv_resnet152', 'tv_resnext50_32x4d', 'twins_pcpvt_base', 'twins_pcpvt_large', 'twins_pcpvt_small', 'twins_svt_base', 'twins_svt_large', 'twins_svt_small', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'visformer_small', 'vit_base_patch16_224', 'vit_base_patch16_224_in21k', 'vit_base_patch16_224_miil', 'vit_base_patch16_224_miil_in21k', 'vit_base_patch16_384', 'vit_base_patch32_224', 'vit_base_patch32_224_in21k', 'vit_base_patch32_384', 'vit_base_r50_s16_224_in21k', 'vit_base_r50_s16_384', 'vit_huge_patch14_224_in21k', 'vit_large_patch16_224', 'vit_large_patch16_224_in21k', 'vit_large_patch16_384', 'vit_large_patch32_224_in21k', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_224_in21k', 'vit_large_r50_s32_384', 'vit_small_patch16_224', 'vit_small_patch16_224_in21k', 'vit_small_patch16_384', 'vit_small_patch32_224', 'vit_small_patch32_224_in21k', 'vit_small_patch32_384', 'vit_small_r26_s32_224', 'vit_small_r26_s32_224_in21k', 'vit_small_r26_s32_384', 'vit_tiny_patch16_224', 'vit_tiny_patch16_224_in21k', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_224_in21k', 'vit_tiny_r_s16_p8_384', 'wide_resnet50_2', 'wide_resnet101_2', 'xception', 'xception41', 'xception65', 'xception71']\n",
            "\n",
            "\n",
            "NetSet: cornet\n",
            "Models: ['cornet_z', 'cornet_rt', 'cornet_s']\n",
            "\n",
            "\n",
            "NetSet: pytorch\n",
            "Models: ['deeplabv3_mobilenet_v3_large', 'deeplabv3_resnet101', 'deeplabv3_resnet50', 'fcn_resnet101', 'fcn_resnet50', 'lraspp_mobilenet_v3_large']\n",
            "\n",
            "\n",
            "NetSet: unet\n",
            "Models: ['unet']\n",
            "\n",
            "\n",
            "NetSet: taskonomy\n",
            "Models: ['autoencoding', 'curvature', 'colorization', 'class_object', 'class_scene', 'denoising', 'euclidean', 'depth', 'edge_occlusion', 'edge_texture', 'egomotion', 'fixated_pose', 'inpainting', 'jigsaw', 'keypoints2d', 'keypoints3d', 'nonfixated_pose', 'normal', 'point_matching', 'reshading', 'room_layout', 'segment_unsup2d', 'segment_unsup25d', 'segment_semantic', 'vanishing_point']\n",
            "\n",
            "\n",
            "NetSet: pyvideo\n",
            "Models: ['slow_r50', 'slowfast_r101', 'slowfast_r50', 'x3d_m', 'x3d_s', 'x3d_xs']\n",
            "\n",
            "\n",
            "NetSet: clip\n",
            "Models: ['RN50', 'RN101', 'ViT-B_-_32', 'ViT-B_-_16', 'ViT-L_-_14']\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['standard',\n",
              " 'toolbox',\n",
              " 'timm',\n",
              " 'cornet',\n",
              " 'pytorch',\n",
              " 'unet',\n",
              " 'taskonomy',\n",
              " 'pyvideo',\n",
              " 'clip']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "show_all_architectures()\n",
        "show_all_netsets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRFVwUj5tiq"
      },
      "source": [
        "You can also inspect the models available from a particular _netset_ using the function `print_netset_models()`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNrc9U0kToEo"
      },
      "source": [
        "We also offer a comprehensive model taxonomy to help you find the most suitable model for your study. Each model in our toolbox has distinct attributes that cater to various research requirements. To facilitate your selection process, we provide a taxonomic overview of the models available.\n",
        "\n",
        "To see the available attributes, use the show_taxonomy function. You can then search for a model based on one or more attributes using the following functions:\n",
        "\n",
        "- `find_model_like(model_name)`\n",
        "- `find_model_by_dataset(attributes)`\n",
        "- `find_model_by_training_method(attributes)`\n",
        "- `find_model_by_visual_task(attributes)`\n",
        "- `find_model_by_custom([attributes], model_name)`\n",
        "\n",
        "This taxonomy system is designed to help you easily identify and choose the most appropriate model for your research needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hO5e2wMlTItN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Architecture': ['Convolutional Neural Network',\n",
            "                  'Swin-Transformer',\n",
            "                  'MLP-Mixer',\n",
            "                  'Vision Transformer',\n",
            "                  'Multimodal'],\n",
            " 'Pre-Training Dataset': ['Taskonomy',\n",
            "                          'ImageNet',\n",
            "                          'ImageNet 22K',\n",
            "                          'COCO',\n",
            "                          'Places 365',\n",
            "                          'ADEK20K',\n",
            "                          'LVIS',\n",
            "                          'Cityscapes',\n",
            "                          'PASCAL VOC'],\n",
            " 'Training Method': ['Contrastive Language Image Pre-Training',\n",
            "                     'Supervised',\n",
            "                     'Jigsaw',\n",
            "                     'NPID',\n",
            "                     'RotNet',\n",
            "                     'Clusterfit',\n",
            "                     'Deepcluser',\n",
            "                     'SimCLR',\n",
            "                     'SwAV',\n",
            "                     'MoCo'],\n",
            " 'Visual Task': ['Image Classification',\n",
            "                 'Various visual tasks',\n",
            "                 'Object Detection',\n",
            "                 'Panoptic Segmentation',\n",
            "                 'Semantic Segmentation',\n",
            "                 'Keypoint Detection',\n",
            "                 'Instance Segmentation',\n",
            "                 'Pose Estimation',\n",
            "                 'Video Classification']}\n"
          ]
        }
      ],
      "source": [
        "show_taxonomy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTLc3ybMShcf"
      },
      "source": [
        "Or you can find a model by its name using the function `find_model_like()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9t0J7tIrSerQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Netset</th>\n",
              "      <th>Model</th>\n",
              "      <th>Image Classification</th>\n",
              "      <th>Object Detection</th>\n",
              "      <th>Convolutional Neural Network</th>\n",
              "      <th>ImageNet</th>\n",
              "      <th>Supervised</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>standard</td>\n",
              "      <td>ResNet18</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>standard</td>\n",
              "      <td>ResNet34</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>standard</td>\n",
              "      <td>ResNet50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>standard</td>\n",
              "      <td>ResNet101</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>standard</td>\n",
              "      <td>ResNet152</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>timm</td>\n",
              "      <td>wide_resnet101_2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>pytorch</td>\n",
              "      <td>deeplabv3_resnet101</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>pytorch</td>\n",
              "      <td>deeplabv3_resnet50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>pytorch</td>\n",
              "      <td>fcn_resnet101</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>pytorch</td>\n",
              "      <td>fcn_resnet50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Netset                Model Image Classification Object Detection  \\\n",
              "2    standard             ResNet18                    1                0   \n",
              "3    standard             ResNet34                    1                0   \n",
              "4    standard             ResNet50                    1                0   \n",
              "5    standard            ResNet101                    1                0   \n",
              "6    standard            ResNet152                    1                0   \n",
              "..        ...                  ...                  ...              ...   \n",
              "478      timm     wide_resnet101_2                    1                0   \n",
              "487   pytorch  deeplabv3_resnet101                    1                0   \n",
              "488   pytorch   deeplabv3_resnet50                    1                0   \n",
              "489   pytorch        fcn_resnet101                    1                0   \n",
              "490   pytorch         fcn_resnet50                    1                0   \n",
              "\n",
              "    Convolutional Neural Network ImageNet Supervised  \n",
              "2                              1        1          1  \n",
              "3                              1        1          1  \n",
              "4                              1        1          1  \n",
              "5                              1        1          1  \n",
              "6                              1        1          1  \n",
              "..                           ...      ...        ...  \n",
              "478                            1        0          1  \n",
              "487                            1        1          1  \n",
              "488                            1        1          1  \n",
              "489                            1        1          1  \n",
              "490                            1        1          1  \n",
              "\n",
              "[89 rows x 7 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_model_like_name('ResNet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFJtD7dUX5o2"
      },
      "source": [
        "The `find_model_by_dataset(attributes)` function enables you to search for models associated with a specific dataset, such as 'ImageNet', 'ImageNet 22K', or 'COCO'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ggT2UcQ0SNXs"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Netset</th>\n",
              "      <th>Model</th>\n",
              "      <th>Various visual tasks</th>\n",
              "      <th>Convolutional Neural Network</th>\n",
              "      <th>Taskonomy</th>\n",
              "      <th>Supervised</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>autoencoding</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>curvature</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>class_object</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>class_scene</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>denoising</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>depth_euclidean</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>edge_occlusion</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>edge_texture</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>egomotion</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>fixated_pose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>inpainting</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>jigsaw</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>keypoints2d</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>keypoints3d</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>nonfixated_pose</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>normal</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>point_matching</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>reshading</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>room_layout</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>segment_unsup2d</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>segment_unsup25d</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>segment_semantic</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>taskonomy</td>\n",
              "      <td>vanishing_point</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Netset             Model Various visual tasks  \\\n",
              "493  taskonomy      autoencoding                    1   \n",
              "494  taskonomy         curvature                    1   \n",
              "495  taskonomy      class_object                    1   \n",
              "496  taskonomy       class_scene                    1   \n",
              "497  taskonomy         denoising                    1   \n",
              "498  taskonomy   depth_euclidean                    1   \n",
              "499  taskonomy    edge_occlusion                    1   \n",
              "500  taskonomy      edge_texture                    1   \n",
              "501  taskonomy         egomotion                    1   \n",
              "502  taskonomy      fixated_pose                    1   \n",
              "503  taskonomy        inpainting                    1   \n",
              "504  taskonomy            jigsaw                    1   \n",
              "505  taskonomy       keypoints2d                    1   \n",
              "506  taskonomy       keypoints3d                    1   \n",
              "507  taskonomy   nonfixated_pose                    1   \n",
              "508  taskonomy            normal                    1   \n",
              "509  taskonomy    point_matching                    1   \n",
              "510  taskonomy         reshading                    1   \n",
              "511  taskonomy       room_layout                    1   \n",
              "512  taskonomy   segment_unsup2d                    1   \n",
              "513  taskonomy  segment_unsup25d                    1   \n",
              "514  taskonomy  segment_semantic                    1   \n",
              "515  taskonomy   vanishing_point                    1   \n",
              "\n",
              "    Convolutional Neural Network Taskonomy Supervised  \n",
              "493                            1         1          1  \n",
              "494                            1         1          1  \n",
              "495                            1         1          1  \n",
              "496                            1         1          1  \n",
              "497                            1         1          1  \n",
              "498                            1         1          1  \n",
              "499                            1         1          1  \n",
              "500                            1         1          1  \n",
              "501                            1         1          1  \n",
              "502                            1         1          1  \n",
              "503                            1         1          1  \n",
              "504                            1         1          1  \n",
              "505                            1         1          1  \n",
              "506                            1         1          1  \n",
              "507                            1         1          1  \n",
              "508                            1         1          1  \n",
              "509                            1         1          1  \n",
              "510                            1         1          1  \n",
              "511                            1         1          1  \n",
              "512                            1         1          1  \n",
              "513                            1         1          1  \n",
              "514                            1         1          1  \n",
              "515                            1         1          1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_model_by_dataset(\"Taskonomy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDPsOQ2X7TK"
      },
      "source": [
        "The `find_model_by_training_method(attributes)` function helps you discover models based on their training methodology, such as 'Supervised', 'Jigsaw', or 'NPID'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ReNMFkHwSQI3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Netset</th>\n",
              "      <th>Model</th>\n",
              "      <th>Image Classification</th>\n",
              "      <th>Convolutional Neural Network</th>\n",
              "      <th>ImageNet</th>\n",
              "      <th>SimCLR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>vissl</td>\n",
              "      <td>rn50_in1k_simclr_100ep</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>vissl</td>\n",
              "      <td>rn50_in1k_simclr_200ep</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>vissl</td>\n",
              "      <td>rn50_in1k_simclr_400ep</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>vissl</td>\n",
              "      <td>rn50_in1k_simclr_800ep</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>vissl</td>\n",
              "      <td>rn50_in1k_simclr_1000ep</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Netset                    Model Image Classification  \\\n",
              "542  vissl   rn50_in1k_simclr_100ep                    1   \n",
              "543  vissl   rn50_in1k_simclr_200ep                    1   \n",
              "544  vissl   rn50_in1k_simclr_400ep                    1   \n",
              "545  vissl   rn50_in1k_simclr_800ep                    1   \n",
              "546  vissl  rn50_in1k_simclr_1000ep                    1   \n",
              "\n",
              "    Convolutional Neural Network ImageNet SimCLR  \n",
              "542                            1        1      1  \n",
              "543                            1        1      1  \n",
              "544                            1        1      1  \n",
              "545                            1        1      1  \n",
              "546                            1        1      1  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_model_by_training_method(\"SimCLR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIfluynEX84g"
      },
      "source": [
        "The `find_model_by_visual_task(attributes)` function allows you to search for models specifically trained for a particular visual task, such as 'Object Detection', 'Panoptic Segmentation', or 'Semantic Segmentation'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Su3z7_0CSUwf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Netset</th>\n",
              "      <th>Model</th>\n",
              "      <th>Panoptic Segmentation</th>\n",
              "      <th>Convolutional Neural Network</th>\n",
              "      <th>COCO</th>\n",
              "      <th>Supervised</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>583</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-PanopticSegmentation_-_panoptic_fpn_R_50_...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-PanopticSegmentation_-_panoptic_fpn_R_50_...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>585</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-PanopticSegmentation_-_panoptic_fpn_R_101...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_panoptic_fpn_R_101_dconv_cascade_gn_3x....</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Netset                                              Model  \\\n",
              "583  detectron2  COCO-PanopticSegmentation_-_panoptic_fpn_R_50_...   \n",
              "584  detectron2  COCO-PanopticSegmentation_-_panoptic_fpn_R_50_...   \n",
              "585  detectron2  COCO-PanopticSegmentation_-_panoptic_fpn_R_101...   \n",
              "600  detectron2  Misc_-_panoptic_fpn_R_101_dconv_cascade_gn_3x....   \n",
              "\n",
              "    Panoptic Segmentation Convolutional Neural Network COCO Supervised  \n",
              "583                     1                            1    1          1  \n",
              "584                     1                            1    1          1  \n",
              "585                     1                            1    1          1  \n",
              "600                     1                            1    1          1  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_model_by_visual_task(\"Panoptic Segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aYEbqWkX-Ui"
      },
      "source": [
        "The `find_model_by_custom([attributes], model_name)` function enables you to search for models based on a combination of the attributes mentioned above. You can provide a list of attributes to filter the models, and optionally specify a particular model name to further refine your search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wzeDdnGgSZg5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Netset</th>\n",
              "      <th>Model</th>\n",
              "      <th>Object Detection</th>\n",
              "      <th>Convolutional Neural Network</th>\n",
              "      <th>COCO</th>\n",
              "      <th>Supervised</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_faster_rcnn_R_50_FPN_1x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_faster_rcnn_R_50_FPN_3x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_faster_rcnn_R_101_FPN_3x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_faster_rcnn_X_101_32x8d_FPN_3...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_retinanet_R_50_FPN_1x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_retinanet_R_50_FPN_3x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_retinanet_R_101_FPN_3x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>COCO-Detection_-_rpn_R_50_FPN_1x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_mask_rcnn_R_50_FPN_1x_dconv_c3-c5.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_cascade_mask_rcnn_R_50_FPN_1x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_cascade_mask_rcnn_R_50_FPN_3x.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_mask_rcnn_R_50_FPN_3x_syncbn.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_mask_rcnn_R_50_FPN_3x_gn.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_scratch_mask_rcnn_R_50_FPN_3x_gn.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_scratch_mask_rcnn_R_50_FPN_9x_gn.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_scratch_mask_rcnn_R_50_FPN_9x_syncbn.yaml</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Misc_-_cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Detectron1-Comparisons_-_faster_rcnn_R_50_FPN_...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>detectron2</td>\n",
              "      <td>Detectron1-Comparisons_-_mask_rcnn_R_50_FPN_no...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Netset                                              Model  \\\n",
              "557  detectron2      COCO-Detection_-_faster_rcnn_R_50_FPN_1x.yaml   \n",
              "560  detectron2      COCO-Detection_-_faster_rcnn_R_50_FPN_3x.yaml   \n",
              "563  detectron2     COCO-Detection_-_faster_rcnn_R_101_FPN_3x.yaml   \n",
              "564  detectron2  COCO-Detection_-_faster_rcnn_X_101_32x8d_FPN_3...   \n",
              "565  detectron2        COCO-Detection_-_retinanet_R_50_FPN_1x.yaml   \n",
              "566  detectron2        COCO-Detection_-_retinanet_R_50_FPN_3x.yaml   \n",
              "567  detectron2       COCO-Detection_-_retinanet_R_101_FPN_3x.yaml   \n",
              "569  detectron2              COCO-Detection_-_rpn_R_50_FPN_1x.yaml   \n",
              "591  detectron2      Misc_-_mask_rcnn_R_50_FPN_1x_dconv_c3-c5.yaml   \n",
              "592  detectron2      Misc_-_mask_rcnn_R_50_FPN_3x_dconv_c3-c5.yaml   \n",
              "593  detectron2          Misc_-_cascade_mask_rcnn_R_50_FPN_1x.yaml   \n",
              "594  detectron2          Misc_-_cascade_mask_rcnn_R_50_FPN_3x.yaml   \n",
              "595  detectron2           Misc_-_mask_rcnn_R_50_FPN_3x_syncbn.yaml   \n",
              "596  detectron2               Misc_-_mask_rcnn_R_50_FPN_3x_gn.yaml   \n",
              "597  detectron2       Misc_-_scratch_mask_rcnn_R_50_FPN_3x_gn.yaml   \n",
              "598  detectron2       Misc_-_scratch_mask_rcnn_R_50_FPN_9x_gn.yaml   \n",
              "599  detectron2   Misc_-_scratch_mask_rcnn_R_50_FPN_9x_syncbn.yaml   \n",
              "601  detectron2  Misc_-_cascade_mask_rcnn_X_152_32x8d_FPN_IN5k_...   \n",
              "602  detectron2  Detectron1-Comparisons_-_faster_rcnn_R_50_FPN_...   \n",
              "603  detectron2  Detectron1-Comparisons_-_mask_rcnn_R_50_FPN_no...   \n",
              "\n",
              "    Object Detection Convolutional Neural Network COCO Supervised  \n",
              "557                1                            1    1          1  \n",
              "560                1                            1    1          1  \n",
              "563                1                            1    1          1  \n",
              "564                1                            1    1          1  \n",
              "565                1                            1    1          1  \n",
              "566                1                            1    1          1  \n",
              "567                1                            1    1          1  \n",
              "569                1                            1    1          1  \n",
              "591                1                            1    1          1  \n",
              "592                1                            1    1          1  \n",
              "593                1                            1    1          1  \n",
              "594                1                            1    1          1  \n",
              "595                1                            1    1          1  \n",
              "596                1                            1    1          1  \n",
              "597                1                            1    1          1  \n",
              "598                1                            1    1          1  \n",
              "599                1                            1    1          1  \n",
              "601                1                            1    1          1  \n",
              "602                1                            1    1          1  \n",
              "603                1                            1    1          1  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_model_by_custom([\"COCO\", \"Object Detection\"], model_name=\"fpn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q4Eb8lHWT8D"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu01QjhzNRgQ"
      },
      "source": [
        "# Example Study: Exploring the Function of the Parahippocampal Place Area (PPA)\n",
        "\n",
        "The Parahippocampal Place Area (PPA) is a scene-selective brain region in the late ventral pathway. It is believed to represent abstract relationships between scene elements, such as individual relations between objects. However, recent research suggests that the PPA is more focused on understanding how the parts of a picture are arranged or put together, rather than just their meanings or purposes.\n",
        "\n",
        "\n",
        "We aim to test this hypothesis using `Net2Brain` and two popular computer vision tasks, comparing neural representations with model representations for scene classification and scene parsing tasks.\n",
        "\n",
        "**Scene Classification** identifies the scene category by considering the entire scene as a single unit. The network learns to recognize global features characterizing different scene types.\n",
        "\n",
        "**Scene Parsing** provides pixel-wise labeling of the entire image, encoding the spatial organization of objects within the scene. This process allows the network to gain a comprehensive understanding of the scene.\n",
        "\n",
        "![SceneParsing_Classification.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4REwRXhpZgAATU0AKgAAAAgABAE7AAIAAAAiAAAISodpAAQAAAABAAAIbJydAAEAAABEAAAQ5OocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJlcnNjaCwgRG9tZW5pYywgU0VWRU4gUFJJTkNJUExFUwAABZADAAIAAAAUAAAQupAEAAIAAAAUAAAQzpKRAAIAAAADMTQAAJKSAAIAAAADMTQAAOocAAcAAAgMAAAIrgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjM6MDQ6MTkgMjA6Mjk6NTgAMjAyMzowNDoxOSAyMDoyOTo1OAAAAEIAZQByAHMAYwBoACwAIABEAG8AbQBlAG4AaQBjACwAIABTAEUAVgBFAE4AIABQAFIASQBOAEMASQBQAEwARQBTAAAA/+ELNGh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8APD94cGFja2V0IGJlZ2luPSfvu78nIGlkPSdXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQnPz4NCjx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iPjxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iLz48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyI+PHhtcDpDcmVhdGVEYXRlPjIwMjMtMDQtMTlUMjA6Mjk6NTguMTM2PC94bXA6Q3JlYXRlRGF0ZT48L3JkZjpEZXNjcmlwdGlvbj48cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0idXVpZDpmYWY1YmRkNS1iYTNkLTExZGEtYWQzMS1kMzNkNzUxODJmMWIiIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyI+PGRjOmNyZWF0b3I+PHJkZjpTZXEgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj48cmRmOmxpPkJlcnNjaCwgRG9tZW5pYywgU0VWRU4gUFJJTkNJUExFUzwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCADIAZcDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6RooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAGmuVt/E18jN9otYp4xIygxNsYAMR0bg9PUV1Zrgofuv8A9dZP/Q2ryczxNTDQjKm+p2YWnGpzcyOntfEum3L+W0xt5f7lwuwn6E8H8Ca1QwOCDkHpXDMquhVwGVhgqRkGmwLJZkfYLia1C9Ejb5P++Dlf0rio53F6VY/cazwa+yzvaK8xtPihqgWSdfDt5qumIWxf23lxlwpILLEz7nHB5GM9geK7LTPF2k6rbwTQ3PlLcRrJF567NwYZGCeDwexr3IYilNaM45Upx6G5RTVYMAQQQehFOrcyCiiigAooooAKKKKAGSlxExjUM4UlQTgE9hmvPtN+KEym8PiPRfsYtpLhD9iuPtJC25AnkbKphF3J0yx3cDivRKxbrwhoF4uLnSreUee853L1dyC5PqGIGR0OBQBRtviFoU17NbXUraeEaRI5r0rFHOY5TCwRi3PzjGDgnIxml8FeK5/FFpdPe6a+l3Fu6g2k2/zUVl3KXDIuCQe24deat6P4Q0jRpLma3thJcXUssss0oDMfMlaUr6ABm/QZyeat6VoOmaFDJFo9lHapKQXEY64AAH0AAAHQDgUAc5F8UNHnlt4obLUWmuphFbReUm6bLSJuHz4ADRODkg9DjBzVG4+MWgyaXdXGkw3V/cwwSzx26BR5kaRly5bJAXGMg/MM4254rd0PwJomiQwrFa+fNDMZkuJuXDbpGGPQDzX4HHzHuc1LL4E8Mz2pt5dHt2iJPy88AqVKg54XBI2jjHagDGsvibZXguI47C7nubdpRJHarvEarI0as5OMBmjYZAIGCSQOaraf8Y/D91p8Ml1HcW909rFM9su2Rg8iI6xjBySRKmCQFOcZyCB0v/CFeHPMST+x7bckjSg7erFt5J9fm+bnoeRSJ4I8Nx27W66Rb+S0QhMZBK7QFA4z1ARRnrhVGeBQBnN8QrKz8NrrGqWVzbI1/PZeV8uUMcki5diQijEZP3sZIAJJGS9+Jei2AneeG9+zxSCFLnylEMsm5V2LIWCggsPvFRw2CcGtdvCehvpa6c2nx/ZFleZYwSMO5Yu2c5y29s887j61E/gnw487zHSLYSsAN6qVKY2kFcfdPyJyuD8o9KAMeT4q+HoleSRbxbdYhJ9oMQ2EmBZgn3s7ijdxgEEEir6ePdKm0C01S0iurpbyd7aG3hjVpXkQOXUfNtOBG5yGIIHBORm6/hDQJI2jfSrdkbgqV4P7ryv/AEWNv0qSfwzo9zpKaZPYRvaI5kWMk8Mc5bdnOTubJzk7jnrQBzV78UtMh1BrKzsLy6uY7iCJ1+RMCSVYyeWyCpYAqwB5HbJAnxd8OTaXJf2iXl1GivKVgjVmMSRmRpMbuBtHQ4bOBtrci8D+GoHd4NHt4mfGTGCuMOHGMHjDqG4789zWbrnw00HWNJXToo20+3BfItUTJVozGVBdW2jDHAXA/DigBkvxO0VXEaW99K8knlWwSJf9JYTpAwQlgPlkdVO7HXIyBmq1n8WdEeeS1ukmjuo2n3RRAOQI3lUDHDMx8lvuggHAzyM9BF4N8PxXH2hdLg87cr+YV53K6uCPQ71VjjqQCc0o8HeH1kdxpVuC4cPwcPvLlsjocmSTr03tjqaAJ/D3iCz8S6QmoafuETMyEOykgg4IypI/ImtSqmm6XZaRbNBp1usEbOZGAJJZj1Yk8k+5q3QAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAJXBQ/df/rrJ/wChtXe1wUP3X/66yf8AobV4Gd/wY+p34L7RJSqSGBHBzxSUq8MM+vevlT0TitO1L+ytBm0qC/vcwgx2M7aFdkwx9gw2YcryARgHAz3rptHtLez0Cxs7Yu9tFbRxp5yFWZQoALKQCD6ggfSvNPDtvqcWt2FrGn2QvcpI00mpxOZWiaRbg+WrkszqUDLj5WBz0zXrNd+LSg7Re+v9WMaTvqRwxvZgDT55bPByFib5f++Tlf0rTg8Rahb5+1QRXa+sR8t/yOQfzFZ6MJE3RkMp6MpyDS0qWPxNDaWnmVKnCp8SOs0vVoNVhd4FdGjbZJHIuGU4Bx6dCOlXq5vwj9/U/wDr5X/0WldJX2mHqOrRjOW7R5FWKhNxQUUUVuZBRRRQAUUUUAFFFFABRRVbUDeCxlOmLC10FzEs7EIx9CRyM9M4OOuD0oAljnjmjDwusiEkBkYEcHB5+tOLV5jF8O/Ef/CQ296+r+XbrbsTDFdMEjmO5woTZ8yiUq27cudvK07Wvh/4q1CKzaLxAxuYbjMkq3DQkx8FW+4wLKxkwABkMPmGKAPSop450DwusiN0ZGBB/GoZtTsre8htJ7uCK5nBMULyhXkA6lVPJ/CvNh4E1bTUkUXc/wBpv75Yd9rI7JHZtEElVs4CY+dww/iCevHS694c1K/8R2l3YQ2IgXyPNnklPmBY5C+3yyjK+c8HKMpOcnAoA6uOaOaNZInV0cBlZTkMD0INODA9K8wj+HGvWQtYbG9jNrEiYR7+dfIm8uJWuFxncQ0bkI2FO/qOQd3wDoeraZ9putVYxJdRxhbc3Mkp3KXJkbf9xiGVSq8ARjnsADs6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAErgofuv/ANdZP/Q2rt7e8tryHzbOeOePJXfE4YZHBGRXEQ/df/rrJ/6G1eBnf8KPqehgt5GLc+M9Htb+4s2N9LLbP5cv2fT55VVsBsbkQjOCO/etDR9ZtNct3uNOMrJHKYnWWB4mVgAcFXAPQjtXK3WsDwxrupi1u9Fu1vbgXEkNzqyWstvJ5aKQQwOQQikdCMng1ueEVRtNnu/ttpdzXd288z2MwlijYgAIrd8KqjJxk5OBnFeFUowhT5ku3X7zrUm5WZw2kWejpqmlpPqzSt5lv9jdLF0HlRPJ5e5iTtMshJ3fx7eOtd94ptLu+8LX9tp243EkeAqttZ1yNyg9iVyAfeuEsjpOk6hbtfXGjGD7XAEP/CQGVotm4RKEEI+Vd7EBjgHBJ4Fdvr6Sah4JuybqGzaW2DvMJiYlHBYbwAdhGRuA6HOO1b4i/tYP8/8AgEQ0izk30l9ZZ7/TNHmtCs1ta6UjxrH9hjjbfLIVz8m7LLgZLAAdDXo/YVxHhC60bUvED3fh2CzsI4rHyrq0gheFmlLjkqUTITaVDYyd56Yrt658XJ83K+n9foaUlpc1vCP39T/6+V/9FpXSVzfhH7+p/wDXyv8A6LSukr7LBf7tD0R5WI/isKKKK6zAKKKKACiikZgilmOAOtAC0VBaXttf2yXFjcRXMEgyksLh1YexHBqegCtqN6mnaXdXsis6W0Lysq9SFBOB+Vc4/wAQLCwtjL4gt7jSztikRSvneYsgcgr5eTwI3zxwBmumvLWK+sZ7S4G6KeNo3GcZVhg/oa5Wf4Y+HrzSjY3y3t2pdHE1xfSvKuxWVVDlshQHfgcfMT15oAkl+JXhqG1u7l7yYw2hYPIlpKyvtfYxQhcOA2ASuQMjPWoX+KXhxbiFYnvJ4JF3faorGZoh8qsBuC8kiROBn7wHeobf4V6KLW6ivpbuczXUk8ZS6ljFuGkZ9sYDfJ1AJGN2ATV+L4d6BBYWdnDBOkFnIJIlW4fnCooDc/MMRpwe6g0ARSfE3wxFbtNcXdxCEZVKyWM6tyH52lM4HlyBj0UoQcGpD8Q9FbUI7K2+1TzSXq2a7bWQKWLMhYMVAZAyEFlJA455pNX+G/h3XI9uoWsjYbcGS4dGB3SNwVIPWaT8x6CrZ8E6N9ot51hlSS2laaJlncbWMxlPfuxP4HHSgCpq3jy20e+v4LqxuStlC0pKFGeTagYlYw28rzjcBjOc4HNSf8LA8PrJsM90G83yVzYzAOfmyVJXlV2NuYcLjkipr7wVpWpX1xc3jXr/AGhHVoftsoiQvGY2dU3bVYqSMgdyepJqO28BaJa3ZuBHcTOXZwtxdSSqm5XDBVYkKp81yQMDJHoMAGhoPiPTvEti13pEzSxI/ltviaNgcA/dYA8ggjjoa1axvDPhXTPCWmtYaLHJFbtIZSJJWkOcAdWJOMAD/JrZoAKKKKACiiigAooooAKKKKADIorzLxr4713RfEU2naBHbXEqGLbBMmCwYZO1sjJ54HrWND8VtemhVwtou4Z2tCQR7daHoC1PZqK8cuPih4ghtTPi02jH/LE+v1pjfE/xMM4SzP8A2xP/AMVSuVys9mzRXiy/FXxIGPmW8JUD70duWA/8epLb4qeJLkMY1syAcf6gj/2alzJD5Gz2qivG2+J/iJFYMLPeMceUeM/jT4fib4ikj3lbTGcf6k/41HtI83KV7KVrnsNJXkn/AAsbxKw4htVGepiOSPpupZPiV4hTH+j2gXOCzKR+VXzIXs5HrVLXkZ+JuuLz5dqRtycxlR/PP6Vm3nxpvrViGmsSwO3ZFC0jbuwwCT0zS50LkZ7dRXNeAtfuvE/hODVL5BHLK7goEK4AYgcHoa6WrIOb8Zapqmm2mnRaJLbwXV/qEdoJbmEypGGDEnaGUn7vqKrrpPjon5/FWkr/ALuhv/W4p3jf/XeGv+w7B/6BJXVUAfOmieAviJefFPV9Z8PaudB0yS6LS3ZtjFHesOHdbYs24EhjliAc5B5xXpVorpb7ZX8x1dwz7cbjuPOO30rvwMVwcPR/+usn/obV4GefwY+v6HoYL7RxXiW41y81uSxt9DvItLUYku7JoDPc8dFZ3Xyx2zgsccbetdH4Xgt7fR0hstGk0eKN9q20uzcf9rKM2SfUnPrXEeLLbRNR8SXNmfD6wz/KbrWJ9IluN2QOIgiMGbGOWwo/2q7DwXp2jaV4fjtPD0c0dokh3G4hkid34yxDhTz7AD0ryKySw8dLf16s6Y/GzmLe7m1C90+XVI5m0e4ukNpdTWNsElfP7s4U71DHoSO4zium1LQ4U8Dto8azPHBbJHF9njBfKY2kKTzyo4zyOK4zRtfhaXTbK6s7F1jv4ZYLWGeQtAZjIuAjMRuidDkYwAcqFwK7zxNdpY+Gry4kkuI9qAK1swWTcWAUKSCASSBk0YjnjUjFaaihazZzXhGGRPF1698LyO4kSa4hims/JULLIhk+be287lQY7D613VcH4KlmfxHcpq32uW+iSeCGSa+FwgWOSMSADy0wdxjOcHI9MYrvKwxl/aa9i6VuXQZpVj4hurzUG0PW7TT4RMoeOfTjcFm8ted3mpj6YrSOkeOe3izSvx0Nv/kip/CP39T/AOvlf/RaV0lfZYL/AHaHojy8R/FZyX9lePO3irR/x0N//kij+y/H3bxPop+uiSf/ACTXW0V1mByP9m+P/wDoZdC/8Esv/wAk0v8AZXj09fFGjD6aJJ/8kV1tFAHKLpPjn+LxZpf4aG3/AMkUp0jxkFzJ4tsAB1I0bH85jXVUEZoA+d/Avw++If8Awnmo65pOsP4f0O5vXmAlgwLxSxywtScLu6/NgjIxX0OuQo3HJx1x1pRRQAUUUUAYvivUrjS9E+0W08drmeKOS6kj3rbxs4DOR7DPJ4HU8A1xEfjfXb6yWPSZVkc3MiLcziLDxrtIwYi6Ofm2llIHUbQRXpwkSR2QMpZeoByRS7OOKAPOvEHjTULHQNFv4tTs4WudP+0yKiqxmkwhHlhyokXlsorBzlcUo+Imo2UDLqNvYIXnlENzc3JhiCLLOo3nYdp/cqoxnJcd+D6EoVhx0B49qTzIyxAdMqcEZ6GgDkPCXjyXxNrL2T2UUG1JmYJOXkgMcioFlXaNhbdkc87G/DtKrWdhb2Uci2sYjWSV5XA7uzFmP4kk/jVmgAooooAKKKKACiiigAooooAKKKKAPCviTo+pXPxKluLK1uJImSIF0jJX7oHUVxM8s8d06opAjkKlQwAOG+noDx79a9k8XaneQ+KzaWs8NvHsjdmYZdsMpKgdMFA4z1zivHNS8LW9x41vbi/1FLO32xlRtkyxdpP7hH9zr9PShtWuJJ3LMeqGB1Ux743X50R/vexyOKlm1lkjZn3qAu7aUBHbjrnpk1jy6NpIs7G7tZ9QDXM7wlBfMQuApB45zz0zUzaFei+sD/aljPHLbPI9vBfvLJGAoILqR8p+Ydz/ACqIyjJ2SNXzrqbUBuLuKGeEBUKhuMkMDg57dqqWc5h85AXXeWwVXOD659vSvQ/C2kQ32jRiBOEhXI7Y25GO+Pr6VyWmaNLNYXF/uUW8MpR+eQSMg/Sk0nJWLUrRd2VSqG2LpGULMNzMcsx9/wClX9L8RW2kQNBOhDElt6qCcccZP0NRRxiayjZL5L5XAZZFxwMdOK5/X5LizndoNpMce4KVzng1z8yVTmZ0cjcOU6SbxfHeKw0vS5pT/wA9HQRrn3Zjn8gaosmt6hE5LW1kD1MCmVwP95sD/wAdNZGj6tbaT4Nur/VGlV2vpILcFGkyAo5JA4HB9K3dN8YaTNpAisJY72/kkWOOzQ7Hcn3fAGPrTlOTdkLkUdzLfw359sJNQknu+fuzTE/mowv6Ve0zRUVTb2dvHEh5KogXpXdWekiSzsf7Qt1tZ5Bvnt2lV9nzEYLDjtWXZfZbe5u186OKOB5OS/3UUbs59AM1yyrLn5GdEaV4cyPQvhvF5PguBCMfvZD/AOPGurrlPhzd2t74QjuLC5W5geeXDqcjIYgj866uvTj8KPLl8TOV8b/67w1/2HYP/QJK6quV8b/67w1/2HYP/QJK6qqJErgoej/9dZP/AENq72uCh+6//XWT/wBDavAzv+DH1O/BfaJKUfeH1pKB94cZ56V8qeieY2+laobizNjcXOn3nnp5skXhoRRoufmAk8vOMcDPUda9DMEOp6T5F9F50NxCFljljKFgRzlTyp9uorgtK0po73S447S1t9St7sPd6ul9HI10uTuGA29t/wDdYALnj7or0iu/Fy1Vv0/QxpLRmbp3h/TNJvLi7sLUR3Fycyys7Ozc5PLE4GeeO9aVFFcUpSm7ydzZJLY1vCP39T/6+V/9FpXSVzXhBlY6mVII+0gcHP8AyzSulr7zBf7tD0PGxH8VhRRRXWYBRRRQAUUUUAFFFFABRRRQB53J4T1yDTZoNN03TbW82mN9SgvJIp7oNKrMxZFUqxALHJb5jjkE5gg8I+NItLgaXWppL0BEuA2pTbJYlhjDKDj5WZ0f94BuG7PtXceINdt/Dukm/u4pZU82OEJEUDFpHCLy7KoGWHJIFZNv8QtDnsGuHlkidVlZoNnmOvllgRlCy8mNwuGO7acZwaAM3w1pOs6f4hitLlriOyiSa8kUXEsiCWSaXYnmMB5g2Pkj+8gY8nJyZfh5rEmsSXNvHa2qG6W4DJcncW+1Ry5BVFYjaHJWRn+YgAgc11moeNdP02TTla2vJjqMPnReXGAQvH8LsrM3zD5EDN7Ulv8AEDw/PYyXT3jQLFIYnSWJtwILjoAcj925yMgBWzjBwAc1D4R8aHezavLAFs5Vii/tSaUG4KxAOzHB2krLheiZyBk4HY+FLLVNP0JINbmMtyJJCMzNLtQsSq72+Y4GOpJ9zUul+JtI1q5mt9Lvo7iWEsHCggHaxViCRhgGGCRnt6itWgAooooAKKKKACiiigAooooAKKKKAPCfiHrbWHxLvIZpysayWBRTnABDlun0/QVx3imdZ9e8g3ZtmMELEqMklJJWAJHIGcdKs/GmcJ8X7hX3hVitXzGhYkhWwOPr3/8ArVHaae+p6szRwX13JcfvEVYYmKqT0bcVOABjhs1MpK1ioxd7mE1y40PTYn2xqtxJlmcYA2rjnj0xWg2rpb+UFaHzI7cxOwjK8cLjPOePar11p1rBdWserfIApkiBsTDGDzyG2gN25DfpVSXRRqOjxR2ep28s4UAlZEZicc/cB4Pua59Yv3lY3Vvsu5mx+LtV0+2vP7L8wTuphNxE5dm+6VI57YPTNdpB4t03SPD15o9xJIl5eHzo2CZUKFIySOevpzXBweEtaWf7JZ2UhuF/eDzWEfy54Iz1GaTUfCfiC+t4JJLaKeeMtuIlUYXJ9fQEj60UVy2aZdabrTfOv66Hc+EJIrnTYoBdRXLRJh3iLY3dTncAe/fr1rL8S6ha2njKOxvoYpofKyFH3zlfm/TH60ngHw/qelxTrdJ5KnBy0gJJ/D2ArD8Z+FtQ1Hxbc3UE8ILeWsTNI3A2AH+E45x096rnTlqKSbgrFq+1HTrzw2sVtZrC8d4iyIcsxVlLEkA/L6Z7YPriuz8ARaNd2wh1O1kkWSEMJIDxhVB5TGRzj9K4DTPDt2rSzX11YoJV2SuZXVyfocA/p9a6SK50zQdIjnuL13mVG+S1Od33d2CGOTtUcE9/ylO7tEialbXcn8V3WlXOoCWcW5SPTY54o5Wd8FnbJDrtG3cwAzyeMisDS9UvLdLmHS7SKMXVvMQAMoSwRWxknbgFu/UVW/tDRJ7yePSTZqs/7qFZpjDsiGGAdZAA3zDkKeRx35kvYtR1KbT4o5lkXT7ZkjSKVFEJZt2NhHIPHIz264rXljy3aMHKd7Jn0P8ACm2+yeA4IgYyonmKmJiykFyQckDNdnXl/wALfEekaH4DtrPX9ZsdPu/OmfyL27SOQKZDg4Yjjj0xXY/8J34R/wChp0X/AMGEX/xVaLYRU8b/AOu8Nf8AYdg/9Akrqq4DxP4n0DV9Q8NW2la5pt7P/bcLeVbXccjYCPk4Uk45rv6YCVwUP3X/AOusn/obV3prgofuv/11k/8AQ2rwM7/hR9TvwX2iSgdR/So1nWWXyrcPcSf3IVLkfXHA/Eir0GiatdrkrFYqT/y1PmP/AN8qcf8Aj1eBRwVet8ETulOMPidjgz4ZvrrVLGZtF0DTFtbtbk3NoS8rBc/KB5S43ZwTnj611guUeTy7cPcSZxsgQuR9cdPxro7fwtZK268aa8bGMSthP++RgfnmtmG3ht4ljt4kijXoqLgD8BXuxymdSzrS26L/AIJxvFQj8KucpBomq3JO5IbNOxlbzG/75Xj9a0rfwrZKyteyTXjr2kbCf98DAP45rdwKMV6dHAYej8MdfM5p4ipLrYigt4raMRwRpGg6KigAfhUtFFdxzhRRRQAUUUUAFFFFABRRRQAUUUUAUNZ0az12wWz1GPzYBNFMUOCGMbhwCD1GVGR3FZF74A0G91aPUHs0jljgFuBGigBF37QOMrjzG+6RnjOcCumooA5/U/B9nq9nFa3t3etCtuLaVFmAWdBj7wxgNx95Qrc8GoZPAmmNCqQz3lu6rtWWGba4GZiRnHf7RJ+hGCAa6aigDmPCvgq28MF3Sd533ziAE/JBFJMZNij/AL5ySTnb2rp6KKACiiigAooooAKKKKACiiigAooooA+c/ivb4+KV9Kv32htwSfTbjH61Rj8SS6Jflo7aO4SaBoZFdip2k84I6Eg11fxEshP8Q7tj1aOIdfRRXDeJLdYJIeACxbIzn0rhqz5LtPU9CjDnspLQ3ofGGhzazFqVzbXlrcJEsKRtiWEKPoQRx6D9auwf8IvcQKv+iaiqTSzeRJAkiyhxnaN+H3AgHIGeorzl+31FOfa4AkG761H9oVWrT1LlllG94XRv6Zp8k3jGWzbTpbmwErRLDFK0awfOeR0GBxxjp6V09p4EhvvElw9/ax2duCcRzRKsSJkkAOSQ7fdGQPu5zg15vE7jcqyyBQ3CiQgDge9OYZ+8SfqxNFPHKC5eVGVTL5TnfnPXV0Hwlp2oWz3+k6BMjRFJ9ohwrbiVYAv6HB9x+XPa/wCGPBvkpdW13oAuMnzoxIuB8y42opC5wG6sOo9MV59KibG+RenXFVmUbRwOmf0qljn0iif7OVrObO9kfwtpOmpCt9ot6+0GQxWSo27IPXacgEeoyD26nB8S+KbG5sL/AE3Roo9l3cFg6qTHBH5aLhARnJ28nsB3zmuYYZP1plh/yFot2MbyeenSlLEyn0NKWEjTd73My4tmchyglVT94AHH+H6VUWJNjMkPRvvRsQVrutQ0u2uQWWJ4JT0lQ4rlr3SZ4UJKibknehwx9yOh/Cs6dRS0OirB7pH0z8Bf9L+EtjJdHz3FxOoaT5jgSHAya9J+y2//ADwj/wC+RXnHwATy/hHZKM8XNx94YP8ArDXplelHZHky+JnI+NII47jw0Y0VT/bsHRQP4JK66uV8b/67w1/2HYP/AECSuqqiRCKxYvCmnqzm5826DOW2TP8AIuSTjaMA9e+a280ZFRKEZ/Er2KjOUfhZHDbx28SxQIscajCoigAfQCn4paKskKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACgnFFFAGBP420K28QNotzdSw3qKXcSWswjVQMljLs2bffdjPHWtCLXdKmnghi1KzeS5UPAi3ClpVOcFRnkcHkehrH8SeC4PEd3LcS3ckDSWq24CIpClZlmVsHg/MgBB6jNYVr8MXj8S/apr/NmPJmPlxqskky3FxO2MD92u6ZcbTkjIPrQB2MfiDTpdel0WKZ3voUDyIsLlUBGRufbtBI5wTmnX/iDS9MvbWzvr6GG5u3CQQlsu5Oew5xweenvWHp/gG10/wAUQ6xHdyyGANsEqh5mJjCHzJzmSRcDIDk4OOcBQK3iT4bWviPWJ72a/mgjuhH58SRqWJRJEUq55XiU8c8ge+QDdPi/w+LmKD+2bEvNBJcJi4Uq0aEB23ZxgE+vr6HD77xVoWmWj3N9q1nDElubokzKSYv74A5I9xXGP8HbN9PitzqtzuQNufnLZeB1OS24YNsn8XQsBjjEr/CS0eyktBqkvkTWpgk328cjljE8e5XbLKMSE4B7dcE5AOyHiHR2hllGqWRjgCtK/wBoTEYb7pbnjORjPWp9L1S01rS7fUdNlE9pcJvikAI3L64PNcSPhParqkmojUpvtP2n7VHlN0auZfNYFGYjaWJwBjAxySM11nhnQ08N+GrHR4pmnSziEYldQpbHfA4FAGrRRRQB4/44WQ+O7rZjBjj5J/2RXnnie1uFYXMyu4LEARDeVGOpA5x9M16R41c/8J7OmFxsjyT/ALorKltUuiQ2CM9u1ePipyTaPawqXKjysuskYkjZZE3DLIcgc9D6fjSk5xXbav4JtrpvPhDLMP8AlpESj/mOv45rkb7SNT0923Ri7X1VfLcfh90/pXGpKWmzO7YpxN+8b/e/oKkLDqDWcL1ElYGO5BJ+6YTkfnx+tI97L/yxtJ356uVX+RNaKnIwc1cvTP8AI30qqzgZFQPLqE6lUtETd3aQt/IChNK124YlIjz/AHIM/qc1aiktWQ5PohHk5xUvh+MXOuIOoQnI9z/9b+dMbwlrMnM8roD2MgX9BXUeE/DEtmrzN8wXjIycmtafK3ZO5nLmSu0W7u0McZEakCuX1AtExUfqeK724gfyz8vfnNc3qtrGN7SYGPfFDiuiFz2Pb/gcc/C6zJGP9In/APRhr0SuA+CoVfhnaBOnnzdP+uhrv69Sn8CPJqfGzlfG/wDrvDX/AGHYP/QJK6quV8b/AOu8Nf8AYdg/9Akrqq0IENefDxnqllf3EcgiuYklcKrjaQAxHUf4V6Ca8ev/APkJXX/XeT/0I1z16kqcLx7nPXk42sd3Y+OtNuAou0ltHPUsu5fzH9QK37W9tr1N9pPHMnrG4b+VeO0qO8T74XeNx0ZGKkfiKwjjF9pfcZRxDW57TRXmNl4v1ez2q0y3MY/hmXJ/76HP55robLx9ZyDF/by2x/vJ+8X9Of0rqjWpz2ZvGtBnW0VUs9Usr9A1ndRTZHRWGR9R1FWhWptuLRRRQAUUVQ1u+k03Qr69hVWktoHlVW6EgZ5o2Av0V47/AMLe1r/nxsP++X/+Kpf+Fva1/wA+Nh/3y/8A8VWHt4Dsew0V49/wt7Wv+fGw/wC+X/8AiqF+Letu4VbGwLMcD5X/APiqX1imFmew0V5bqXxM1XS7GOW8trOKV+AjISpPfBD1n/8AC3tZ/wCfKw/75f8A+KrKljaNaPNB6G9TDVadnONr7HsVFeEr8btfaQD7Dp/LY+6//wAVWp/wt7Wv+fGw/wC+X/8Aiq2+sU2dWOyvE4BxVdW5ttbnsVFePf8AC3ta/wCfGw/75f8A+Ko/4W9rX/PjYf8AfL//ABVHt4HnWPYaK8e/4W9rX/PjYf8AfL//ABVaWmfEjWrvxFZ6dd2diizzIjmMNkBsdPm6801WhJ2QWPT653xT4wg8K/YzdWVzcJcuwaSIDZCqgEs5Jwo56nj1Iroao32lafqgi/tKxtbwRNvjFxCsmxvUbhwfethHI6h8WNP0/PmaXfSb7hoLcReW7TFGmDHYGLKB9nkxkDPHvgm+K9hC027Sr/arMkLBATMyzRREbQSw+aePqM9eMjB6eTwxoc/nmbRtPkNy4kn32iN5rDozcckZPJ9all0LSZomjn0yzlSQMHV7dWDBiCwORzkqpPqQPSgDEn8f2kej6Xfx2F2x1GKSZYZNkLRJGMuWMjKoI9M8/TJrNf4sWG1PI0q9lM8hW3AMY81Q8kZb73y4aJuDg4ORnBA66fQ9KurOC0udMs5ra3IMMElurJGRwNqkYGB6UieH9Ijklkj0qyV5pBLIy26Au/PzE45PzHnryfWgDjh8XbFrdZl0i92m3e7ZWeIFYVhimLfe5OyUfKOcgj3q2/xMtRctBHpd3I7zmC3w8Y84i4+zseWymJD/ABYyOR3FbVz4M0C71Czu59LtWazVlij8lNgyEAJXGMqI0APYDFXU0HSo7iW4j02zWaaRZZZBbrukdTlWY4ySDyCelAHCRfG7RrixmuoNL1N0ht1mf9yAAxjWTZuztztcc56/gaszfFGWG+mQ6BdyQwx7WWIiSUTC5kgK7VJyo8stkZOO1daPDOhjP/El07PleTxap/q/7nT7vt0p1x4b0S83G70iwnLbgTLbI2dzb26ju3zH1PPWgCzpeoRarpNrqFtzDdQrMnzA8MARyCQevY4oqeGGO3gSGCNY441CoiKAqgcAADoBRQBh6j4P0zVNUfULnzvPcAHa+BwMdMVAPAmkqeGuP+/g/wAK6ais3ShJ3aNFUnHZnO/8ITpf965/7+//AFqrXPw60O7XE6zt/wBtP/rV1dGKj6vSf2UV7er/ADM4U/CLwuTlorg/9tf/AK1SR/CjwvH0tZT/AL0mf6V22KTFL6vS/lQe3q/zHKR/DnQIh+7gdfowH9Ke3w+0Ruqz/wDfz/61dTijFH1aj/Kg9tU/mOTPw40E/wDLOYfRx/hU6eA9Gjs/syLOEzn/AFnP8q6WjFXGlCDvFEupN7s46T4Z6FICGN5g+k3/ANaqkvwe8MTqVkW8Ibr+/wD/AK1d5RVeziugueXcyPDXhux8KaLHpel+b9njdnHmtubLHJ5+prXooq9iNzlfG/8ArvDX/Ydg/wDQJK6quV8b/wCu8Nf9h2D/ANAkrqqAErx6/wD+Qldf9d5P/QjXsNePX/8AyErr/rvJ/wChGuTF/wAP5nLiNkV6KKK8s4worm7rUfEUOt21gkelH7UsrxsTL8oTbweOp3D8jWjp2q+d4cTU7/ZCBE8k2zO1QpOSM844zVOLtcrlaVzTB2uHU4YdGHBFa1l4p1iwG2O7Myf3Zxv/AF6/rXGSeK7eCKFrmyvImkjM0kZRS1vFuwJJADwD6DJ68cVu9hWkalSnsx3lE9R8Na4+uWMkssKxPE+xtrZB4Bz7da2a5H4e/wDILvP+vn/2Ra66vXi7xTPQptuKbCsnxSM+E9W/685P/QTWtWR4q/5FHVudv+hy8/8AATVFnzyUHak2GpDyD2rlNb8RXukmBIAjhwSS4JPH416GX5HUzJyVG1130OqFByw8699I2v8AM6fYaAh9cVgaP4iuNUtpGaNUljOCMcH0rY0u6lvdNhnuUVJWBDqvQEEj+lGPyCtgI81a1r20Y62HnSpQq3vGezXkQa00jQxb3LfMcZPTiriqwhBxn5c1T1n/AFMX+8f5VfLf6Pn/AGM/pXgqjDnkl0PbxF5ZXhP8U/zRzif6xfqK6LYa51Pvrj1FdNWdCmp3ue3xnpOj6P8AQj2GnbB35qQxuEDlTtPQ0jIyHDAg9eRXVGnS6M/PPaReiY0DHStnwvvbxbpbtkj7XGCx/wB6setbwsT/AMJZpQzx9rj4/wCBCt7JbFH0JXhXj/XdVtvHF/BbaldRQxlAiRzMoUbFPQH1Ne61xeu+CvDGq6xLe6pO0V1KBvAuAucDA4PsK9TLcXh8JWc8QtLWPYyfHYXA13UxSvG1uj107njf/CSa3/0F77/wIf8Axrt/hVrOpX3iieG9vri4j+yswSWUsMhl55+tbn/CuvBX/P6//gUv+FbXhnwn4e0PUJLjRZjLM0ext0wfC5z0H0r18ZnGXV6EqdJe89tEe7j8/wApxOGnRor3mtNEJJ4ture3vNRns7f+zLaa4hG25AnZomZeEYBTuZTgbuhB7nGRB8Wba6tp7uHSb1LVLYyCWUICkoM6mN13ZBJgYAjI55xXUT+D/D13fXV7c6NYy3N5GY7iVoFLSqcZBOOfur+Q9BUUHgTwvbSRvb6Dp8bxRPCjLbqCqNu3KPY73/76PrXyp8UYl18VdMsUkN3puoRES+XCGEQ+0cygsv7zAA8iT72CeMDmt/wv4jHiWzuruO1eCGK6eCIueZUGMPjqM56HkVJeeE9B1C3MF7pNnPEcZSSEEcMzfzdj/wACPrV6y02z07zRYW0Vus0nmSLEgUM2AM4HfCgfhQBaooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDlfG/+u8Nf9h2D/0CSuqrkPiDdQWSeHrm8njggj1yBpJZXCqg2SckngVq/wDCZeGP+hi0n/wNj/8AiqANmvHr/wD5CV1/13k/9CNamm/HTwy/iS60DX7iHTLyCZkjuBMslrcJ1V1lHC5XBw2MHjJrJvJFlvriSJldGmcqynIYbjyDXJi/4fzOXEbI5o3+uXWsajbacNPSGzkSMG4SQs26NXz8rAfxY/CrWiX99dzahb6ktv51nOsWbYMFYGNX6MSc/NisjxKlxbXk+ox232aJEVXvI9U+zb1HQMuwg4JIHf09KveEWkk0Vnk0uTTvMlZgJ5jLJNkD94xIDZPvzjHsK4ZJclzBpctzJuNaj1XYdU04IsdyqwyQXbpKivK0B5XB3BgMqDjDe1dJeWVhD4dnspQLewS2aNwvSOPaQefYVy8un2Vhq1vdWj6DPdPfBPL+zBZid437XMpy67s9M5FdVrbwR6BfvdxGaBbaQyRg43qFOR+VOdtLBK2ljmrSzXVZbm31G71BLm/jWMy3FmIfPgjzlE64J3kt0PPQV2dcfo11eSeIraDWlSaa2WS3gljkLYYRxuzNwMkrIF3cdOnzV2FTVvewp3vY7z4e/wDILvP+vn/2Ra66uB8G69pGlWNzDqeq2VnI8+5UuLhI2I2ryAT0rpP+Ey8Mf9DFpP8A4Gx/4168PgXovyO2l8CNqsnxUM+EdW/685P/AEE1Tv8AxtoUOnXElhrujz3SRM0MT38ah2A4UnPGTxntXD2Xxr8KeNvCmp2kF0dN1RrR1+xXhCszFSMI3R+fTn2FWann571554rjk/tK3bnYy4B7A5//AFV1Oo6/DFbyrZzA3CtgZQ4689sVu2egW2paTA+rhbp3AlGAUCEjtg16eX8TYfIJOpXi5KWmm/UyxmZ4fL8K6eJ0U2rfLU4S4uptHuZUt7bzmupN8eB1OORxz6VtSXtyul2iufs9w0e+VOhX0FdmmkWcedsX09vxpX0HSJ7dvMtgboniV5HwR9Aeo9f5142d8cYbNKSoU6bj7277fp+JtlvG2UVsdFV3KNJXaTStd9/vZyeqSrNa27o25WJII78VfznT8j/nl/Srus6J5tpbQ6XaW0Qh4bYMNIfUseTWz/Zel/8ACJ+VtcaqIPvbiQZMdPTFeB/bOHp6z1vpprbz9D1sRn2TVsFh40K8bRnLRuzSb6rt5nnlsA1xED0LgfrXSVi2mnXT6gsCxMsiMNxI4X3zW5HpGoo2DJHj3Oa9jB1cIqc3VrRi1sm9/Q93i/H5dOrT58TCLUb2b3T7WuNpSxIAJJCjAHpWiNKG0ZlOcc/LVO+hSxKb5Mh844qMLmGGxdZUaD5pa2Vmfm2DzbB42uqGHfNPWys/8iGtbwt/yN2lf9fcf/oQrF+0Rf8APRfzrW8KTRt4t0ra6n/S4+/+0K9d4atFXcH9zPceGrRV3BpejPoevJPF3/I1XuOm5f8A0EV63XI654m8Kadq8trq6K12gXefsxfGRkc49CK8XMsK8VSUOa2tz5/OMtlmNBUoytZ3POK6r4fnHiCX/rg38xV3/hNPAv8AzwX/AMAz/hWx4c8QeG9WvZIdCRUnWPcw8gx5XOOuPUivJwmUOjXjU9onY8LAcM1cJiYV5VE1F32ObXxvd2n9oXU2s2z3kLMk+jzQhU01fOEayyOo3KgU72LEhhyu0U62+KFxc2jp9it49RwzwwNKwWaMQSOJlOM+Wzx4DY5U+tejBRk+/Wl2CvqT7Y8g1L4qeIdHu743dlYyiDZB5aTBI0cT3MbyF3K8N5CKATwXXqeD6F4V1nUNds7i7v7RLJVuXhih3FnUIxB39s5GOMjit0op7UoAHSgBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAhuLSC7j8u7hjnjznZIgYZ+hqt/YOkf8AQLsv/AdP8Kv0UAed6Z8F/DUHie68Ra1Amr6lPMZEE8YEEA6KqRDj5VwMtnkZ4rAvVCahcqoAUTOAAMADca9irx6//wCQldf9d5P/AEI1x4v+H8zlxOyOE1+0lPiI37Sasy26DyhHZxSwRccsu84z/tYyPWtjwtqbatpslx9purhfNKpJdWyQ5AA+6E4Ye/1qpruhazq2pho72yOnx4K2VzC7K7YHL7WG7noDx7Gta0ku7DTpJNdubFRHyHgRoo0QDjO5jXHJ3gl1MG7xscvb+HpFvws9tqTRJOnlP9niASMTmUgnzSTlyuWABwnTNdfcSCTTJnkYWwMT7mnUERjB5YZwR3xnpXEzR2F7r0NraaXpdleJeJJ9pF2hf5XDEbQMliARt9/au11OS5h0m8lsU8y5SB2hXGdzhTgfn2oqXurhLocroAgtfE0Men39hqCTxOJ2s7VUMQRVCbmVjheAMewrtK4/w7eXr+IJoEvry/tixaRrpMBUMUTIw+UYJZnG307DFdhU1fiJnudn4G02xvdOunvLK3ndbjAaWJWIGxfUV1H9haR/0C7L/wAB0/wrA+Hv/ILvP+vn/wBkWuur14fAvRfkd1L4EZF/4csJ9PuIbOysba4kjZY5jao3lsRgNjvg84rz5vhL4W+H/gHXLvTLQ3Oprp8zHULsiSXcEPK54Tn0APqTXrFc/wCPf+Se6/8A9g+f/wBANWbLc+QCctk8nrk167pZzo9n/wBcE/8AQRXkVet6O2/Q7E+tun/oIr5HiL+FD1Z8lxqv3FJ+b/IuUUUV8afmIUUUUAFFFFAXbCsPxIybIF3qHyflzyR64/CtyvPfHUmdfiCtzHAvTt8xr6Xhes8PmlOulflu/wALfqfYcFzdLOqVZfZu/wALfqWK2PCX/I5aR/1+R/8AoQrh7fVbiDAc+avo3X866zwTqcFz4y0dclJDeRYVh/tDvX79HOcJiqE4p2lZ6P0P6U/tTDYmhJJ2dno/Q+pO1eJ/EHStQn8cX80FhdSxP5ZV44WYH92o6geor2we9cR4h+I6aBr1xpv9mNOYQuZPO25yobptPrX5xOHOrH5qjyX+xdV/6Bl7/wCA7/4V2/wr02+tfE1zLdWdxBH9kZd0sTKM7l45Hsa0P+FwL/0Bj/4E/wD2NbnhTx2nijVJLP8As9rZo4jKG83fnBAx0HrWUaHK73HcXVdKu5vFxupdMur+Nvs/2OeG7WJLTaxL7hvB56kqrbh8p4FcvcaX8QdRt7cXweR45YWfd9nQgrPbs+0qxyhVJCAcHAII5Ar1OWSKCJpZ3SONRlndgAPqTUaXlnJIY47mFnEhiKrICQ4XcVx67ecdcc10iPNxL4x0PRIbnXL64hhDMtwsb2imFRLGIwjMAoypfJY47cHFdt4Pur698GaRdasXa9mtI3nMiBGLlQTkAAA+2BWotzbPcNAk8TTLndGHBYYCk8dejL/30PWpqACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigBK8ev/wDkJXX/AF3k/wDQjXsNePX/APyErr/rvJ/6Ea5MX/D+Zy4jZFeqeq2B1HTnt0k8p9ySRvt3bXRg6kjjIyoyKuUV5ezuci0ZyUHhnWUs4LGfULF7SO9F2wS1YOf3vmYBLnHPfGa62iiqlJy3G5OW4ZJ6milRWkkEcas8jdEUZJ/AVtWXhHV73k24tk/vTnH6Dn+VXGjUnsgUZS2Oh+Hv/ILvP+vn/wBkWuurH8O6GdCs5IWm85pH3sQuADgDHX2rYr2Iq0Uj0aaaikwrn/Hf/JPtf/7B8/8A6Aa6CsDx3/yT/X8/9A6f/wBANUaLc+Pq9P8ADWoWlzo9rBDOjTQwqrx55UgY6V5gKWORopA8TlHU5DKcEV4+YYFY2moN2aOTOspjmlFU3Llad1/wT2eivOdN8a6hZKI7nF2gPVzh/wA/8a6zTvFel36DM620neOY4/I9DXxmJyrE4fVxuu6PzDHcPY/BXbhzR7rX/gm1RWZJ4j0eMkNqEJx/dO7+VUp/GujxA+XLJMc9EjP9cVywwWJn8MH9x59PK8dVfuUpP5M6CiuOn+IEI/49rF3/AOukgX+WapTePr5v9TbW8f8AvZb/AArthkuMn9m3q0erS4XzOp9i3q0d47rHGzudqqMknsBXk+s3/wDaesXF0udjthAeyjgVPd+JtWvUeOa7IjcEMiKFBB7cVlZr6TKsslg3KdR3b7H2/D+QzyyUqtZpyeit0XUK6DwH/wAlC0H/AK/4f/QxXP5rf8Cf8lB0H/r/AIf/AEMV7yPrZbH2BXlvjHwPrmr+K7u/sII5IJtm0mUKeEAPB9xXqVeGfEX4reJvDnjy/wBJ0uS2S2thGFDw7mO6NWJJPu1at2OGMXJ2RL/wrXxN/wA+cX/f9f8AGup8AeENX0DXZ7rU4Y442tzGu2QMSSynt9K8p/4Xl40/5+LT/wABhXd/Cb4k6/4v8T3On609vJClo0yGOLYwYMo9fRqSkmW6Ukrs9W1XT4dX0a8065yIbuB4XK9QGBGR78151efDDU5bZjHqkTT3EO66VgQstw8u6ZwSrAAoNgyrcdRXb+JvElt4W0gahfRSyxmVIQkTIpLMcDl2VQPckVSsvHmhzrGLyf8As6aSVYo4bp03MWSNgQUZlIxNGM5xlgO4zRkVvCHhK90Ga3kv7yO5kitfIdlzl28q2TPP/XAn8RXX1i2vjDw9e3EVvaatbTTTSGOONGyztgHgemCDnpjnNbVABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAhrx6//AOQldf8AXeT/ANCNFFcmL/h/M5cRsivT4IZbqTy7aJ5pP7kalj+lFFcdCnGpKzOWKu7G7ZeC9Wu9rTLHaRnqZWyw/wCAj+pFdDZeBNOgO68klu2/uk7F/Ic/rRRXpRpQhsjujSglc6C1sLSyjCWltFCvoigZqyKKK1NkFFFFABVHWtMj1rQ73TJpGjjvIHhZ0xlQykZGe/NFFAHlv/DO+i/9BvUv++Y//iaP+Gd9F/6Depf98x//ABNFFKyL9pLuH/DO+i/9BvUv++Y//iaP+Gd9Fx/yG9R/75j/APiaKKOVB7SXcP8AhnfRe+t6l/3zH/8AE0f8M76L/wBBvUv++Y//AImiiiyH7SfcP+Gd9F/6Depf98x//E0f8M76L/0G9S/75j/+Joooshe0n3D/AIZ30X/oN6l/3zH/APE0f8M76L/0G9S/75j/APiaKKOVB7SfcP8AhnfRf+g3qX/fMf8A8TV3RvgVpGi63ZanDq9/JJZzLMqOqbWKnODhelFFFkHtJdz1IdK8s8ZfBZfFviu71oa61obrZmL7J5m0qip13j+7npRRQyVJxd0Yf/DOQ/6Gg/8AgB/9srq/h/8ACdfAmuT6l/bBvmltzAE+zeWFyynOdxz92iijlSKdSTVmdtquj2usw28d5uKQXCXCqMYZlOQDkHisi+8CaZfX8t0bi8g+0PmeKF1VJo9kamIgqfkxCh4wevODiiimQGjeA9M0XVotRhuby4uYYTbxNcSKdkOAFjGFHC4OCeeTkmumAwKKKACiiigAooooA//Z)\n",
        "\n",
        "**Hypothesis:** We hypothesize that cognitive representations encoding structural relations (as suggested by recent studies on PPA) will better align with scene parsing than scene classification DNN representations. This approach enables us to investigate brain region functions using cognitive modeling.\n",
        "\n",
        "\n",
        "## Roadmap\n",
        "\n",
        "\n",
        "1.   **Loading Models**: Loading Scene Classification and Scene Parsing models into `Net2Brain`\n",
        "2.   **Feature Generation**: Extracting model feautures using the `BonnerPnas2017`-Dataset\n",
        "3. **RDM Creating**: Turning the features from both models into one RDM per network layer\n",
        "4. **Evaluation through RSA**: Comparing model to PPA-representations using RSA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu4sYz1tLpRL"
      },
      "source": [
        "# Load the Dataset\n",
        "> Bonner Michael F, Epstein Russell A. Coding of navigational affordances in the human visual system. Proceedings of the National Academy of Sciences. 2017;114(18):4793–4798. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MSqh60cHUJxV"
      },
      "outputs": [],
      "source": [
        "from net2brain.utils.download_datasets import load_dataset\n",
        "stimuli_path, roi_path = load_dataset(\"bonner_pnas2017\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoD_SmGWUxX_"
      },
      "source": [
        "# Step 1: Using FeatureExtractor with a pretrained DNN\n",
        "1. Scene Classification ([Places365 CSAILVision](https://github.com/CSAILVision/places365))\n",
        "2. Scene Parsing Model ([Semantic Segmentation CSAILVision](https://github.com/CSAILVision/semantic-segmentation-pytorch)) \n",
        "\n",
        "\n",
        "To extract activations from a pretrained model in one of the netsets, you must first initialize the `FeatureExtractor` class and specify the name of the model as well as the netset it belongs to. \\\\\n",
        "Additionally, you can choose the device on which the extraction is computed, with either `cpu` or `cuda`.\n",
        "\n",
        "If you want to implement **your own model**, save the model in a variable and put it into the Feature Extractor like:\n",
        "`FeatureExtractor(model=my_model, device='cuda')`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZSZWRMLWRFLx"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Domenic\\Documents\\Repositories\\Net2Brain\\notebooks\\Workshops\\Net2Brain_PPA_VSS_Demo.ipynb Cell 29\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnet2brain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureExtractor\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fx_class \u001b[39m=\u001b[39m FeatureExtractor(model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPlaces365\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             netset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtoolbox\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m fx_parsing \u001b[39m=\u001b[39m FeatureExtractor(model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSceneParsing\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                               netset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtoolbox\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/Net2Brain_PPA_VSS_Demo.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                               device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\net2brain\\feature_extraction.py:320\u001b[0m, in \u001b[0;36mFeatureExtractor.__init__\u001b[1;34m(self, model, netset, layers_to_extract, device, transforms, pretrained)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m netset \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNameError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnetset must be specified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_netset_model(model, netset, layers_to_extract)\n\u001b[0;32m    321\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_model(model, layers_to_extract, transforms)\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\net2brain\\feature_extraction.py:386\u001b[0m, in \u001b[0;36mFeatureExtractor.load_netset_model\u001b[1;34m(self, model_name, netset, layers_to_extract)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39melif\u001b[39;00m netset \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoolbox\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39m=\u001b[39m toolbox_models\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule\u001b[39m.\u001b[39;49mMODELS[model_name](pretrained\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained)\n\u001b[0;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrained:\n\u001b[0;32m    388\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\net2brain\\architectures\\implemented_models\\semseg_models.py:611\u001b[0m, in \u001b[0;36mget_semseg_model\u001b[1;34m(pretrained)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdecoder_epoch_30.pth\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    609\u001b[0m     \u001b[39m# Download the file using requests\u001b[39;00m\n\u001b[0;32m    610\u001b[0m     file_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttp://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50-upernet/decoder_epoch_30.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 611\u001b[0m     r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(file_url)\n\u001b[0;32m    612\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m~ Downloading weights\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    613\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcheckpoints\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdecoder_epoch_30.pth\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[0;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\Users\\Domenic\\anaconda3\\envs\\N2B10\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from net2brain.feature_extraction import FeatureExtractor\n",
        "\n",
        "fx_class = FeatureExtractor(model='Places365', \n",
        "                            netset='toolbox', \n",
        "                            device='cuda')\n",
        "\n",
        "fx_parsing = FeatureExtractor(model='SceneParsing',\n",
        "                              netset='toolbox', \n",
        "                              device='cuda')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How are the models different?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fTE6x7WxMB3Z"
      },
      "outputs": [],
      "source": [
        "#@title Code for visualizaion\n",
        "# System libs\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import PIL.Image\n",
        "import torchvision.transforms\n",
        "import torch\n",
        "from mit_semseg.utils import colorEncode\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "colors = scipy.io.loadmat('/content/bonner_pnas2017/etc/color150.mat')['colors']\n",
        "\n",
        "def visualize_result(img, scores, index=None):\n",
        "\n",
        "    _, pred = torch.max(scores, dim=1)\n",
        "    pred = pred.cpu().numpy()[0]\n",
        "    if index is not None:\n",
        "        pred = pred.copy()\n",
        "        pred[pred != index] = -1\n",
        "\n",
        "    pred_color = colorEncode(pred, colors).astype(np.uint8)\n",
        "    img = img.astype(np.uint8)\n",
        "    im_vis = np.concatenate((img, pred_color), axis=1)\n",
        "    display(PIL.Image.fromarray(im_vis))\n",
        "\n",
        "# Load and normalize one image as a singleton tensor batch\n",
        "pil_to_tensor = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "pil_image = PIL.Image.open('/content/bonner_pnas2017/etc/ADE_val_00001193.jpg').convert('RGB')\n",
        "img_data = pil_to_tensor(pil_image)\n",
        "input_image = img_data.unsqueeze(0).cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5fD3KtWbNxJ1"
      },
      "outputs": [],
      "source": [
        "#@title Testing the models\n",
        "\n",
        "# Run Scene Classification Model from toolbox\n",
        "scores_scene_classification = fx_class.model(input_image)\n",
        "_, pred = torch.max(scores_scene_classification, dim=1)\n",
        "print(\"Scene Classificaion:\", pred) # orchard 249\n",
        "\n",
        "\n",
        "# Run Scene Parsing Model from toolbox\n",
        "scores_scene_parsing = fx_parsing.model(input_image) \n",
        "visualize_result(np.array(pil_image.resize((224, 224))), scores_scene_parsing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP-p77eFH7Mf"
      },
      "source": [
        "This initializes the feature extractor and loads the model and any specified layers for extraction into the instance. To view the layers that are set to be extracted, you can execute `fx.layers_to_extract`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhXqO0Bx1qqe"
      },
      "outputs": [],
      "source": [
        "fx_class.layers_to_extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntKeDWmuIi9w"
      },
      "source": [
        "Note that the suggested layers may not be exhaustive. To view a complete list of all available layers, you can use `fx.get_all_layers()` and overwrite the `layers_to_extract` attribute with your desired subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3SGIaHXJHpP"
      },
      "outputs": [],
      "source": [
        "fx_class.get_all_layers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssVK6gFQUi-H"
      },
      "source": [
        "### Extracting Layer features\n",
        "To extract weights from the layers, you can use the `fx.extract()` function and provide the path to the images that you want to run through the network. You can choose between the 'npz', 'pt', or 'dataset' formats, but it is recommended to use 'npz' if you plan to use the other steps in the toolbox.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "972RH2jd1uhY"
      },
      "outputs": [],
      "source": [
        "# Create features for the Scene Classification model\n",
        "fx_class.extract(dataset_path=stimuli_path, \n",
        "                 save_format='npz', \n",
        "                 save_path='Classification_Feats',\n",
        "                 layers_to_extract=['model.4', 'model.5', 'model.6', 'model.7', 'model.9'])\n",
        "\n",
        "# Create features for the Scene Parsing model\n",
        "fx_parsing.extract(dataset_path=stimuli_path, \n",
        "                   save_format='npz', \n",
        "                   save_path='Parsing_Feats')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESxU8jO6WQMd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIg8FGmD2cN6"
      },
      "source": [
        "# Step 2: Creating RDMs from Layer Features\n",
        "In Step 1 of the process, the Feature Extractor extracts features that are used here in Step2 to calculate Representational Dissimilarity Matrices (RDMs) through the built-in functionality of the RDM creator. To do this, the RDM creator function requires the path to the location of the .npz files containing all the layer features for each image in a [Batch x Channel x Height x Width] format. The function then produces an RDM in the shape of (#Images,#Images) for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQNy9Egm2iJU"
      },
      "outputs": [],
      "source": [
        "from net2brain.rdm_creation import RDMCreator\n",
        "\n",
        "# Create RDMs for the Scene Classification model\n",
        "creator_class = RDMCreator(feat_path=\"Classification_Feats\", \n",
        "                     save_path=\"Classification_RDMs\")\n",
        "creator_class.create_rdms() \n",
        "\n",
        "# Create RDMs for the Scene Parsing model\n",
        "creator_parsing = RDMCreator(feat_path=\"Parsing_Feats\", \n",
        "                     save_path=\"Parsing_RDMs\")\n",
        "creator_parsing.create_rdms() \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDDuP4AMYOdj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3J0pnmbWBX-"
      },
      "source": [
        "# Step 3: Performing RSA on the Model RDMs and the ROI RDMs\n",
        "This tutorial demonstrates how to utilize the evaluation features of Net2Brain and plot the resulting data. You have the option to select from three different metrics for evaluation: \"RSA\", \"Weighted RSA\", and \"Searchlight\". Each module returns a pandas dataframe that can be easily integrated into the toolbox's integrated plotting functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ6xZij_2rHd"
      },
      "outputs": [],
      "source": [
        "from net2brain.evaluations.rsa import RSA\n",
        "\n",
        "# Get path to RDMs\n",
        "model_rdms = r\"/content/Classification_RDMs\"\n",
        "roi_path = r\"/content/bonner_pnas2017/brain_data_live_study\"\n",
        "# Start RSA\n",
        "sc_evaluation = RSA(model_rdms, \n",
        "                 roi_path, \n",
        "                 model_name=\"Scene Classification\")\n",
        "\n",
        "df_classification = sc_evaluation.evaluate() # Evaluation - Returns a pandas dataframe\n",
        "\n",
        "\n",
        "\n",
        "# Get path to RDMs\n",
        "model_rdms = r\"/content/Parsing_RDMs\"\n",
        "\n",
        "# Start RSA\n",
        "sp_evaluation = RSA(model_rdms, \n",
        "                 roi_path, \n",
        "                 model_name=\"Scene Parsing\")\n",
        "\n",
        "df_parsing = sp_evaluation.evaluate() # Evaluation - Returns a pandas dataframe\n",
        "\n",
        "display(df_classification)\n",
        "display(df_parsing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6AfKmA73OH3"
      },
      "outputs": [],
      "source": [
        "# Comparing statistical significance\n",
        "ttest, sig_pairs = sc_evaluation.compare_model(sp_evaluation)\n",
        "print(sig_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFT3HH9VYWbe"
      },
      "source": [
        "### Visualizing RSA Evaluation Results\n",
        "\n",
        "If you would like to visualize the evaluation results, you can do so using the integrated plotting functionality of the toolbox. To do this, initialize the class with a list of dataframes that were returned through the evaluation. It is important to ensure that each dataframe contains the same ROIs, indicating that each test was conducted on the same brain RDMs. Additionally, each dataframe should contain a different model name, which can be set manually or through the \"model_name\" parameter during evaluation (as described above).\n",
        "\n",
        "The following example illustrates how to plot the data using a single dataframe.\n",
        "\n",
        ">Note: Multiple ways of plotting are planned for implementation in the future. For now, only the \"best_layer\" variant is available, which plots the best performing layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD-iK0DF3hW2"
      },
      "outputs": [],
      "source": [
        "from net2brain.evaluations.plotting import Plotting\n",
        "\n",
        "# Plotting with significance\n",
        "plotter = Plotting([df_classification, df_parsing])\n",
        "results_dataframe = plotter.plot(pairs=sig_pairs, metric = \"R\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkcD1-zMgzSx"
      },
      "source": [
        "## Let us add random weights!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ7IztjyNaXa"
      },
      "outputs": [],
      "source": [
        "## Feature Extraction ##\n",
        "\n",
        "fx_class_random = FeatureExtractor(model='Places365', \n",
        "                                   netset='toolbox', \n",
        "                                   device='cuda',\n",
        "                                   pretrained=False)\n",
        "\n",
        "fx_class_random.extract(dataset_path=stimuli_path, \n",
        "                        save_format='npz', \n",
        "                        save_path='Classification_Feats_random')\n",
        "\n",
        "fx_parsing_random = FeatureExtractor(model='SceneParsing',\n",
        "                                     netset='toolbox', \n",
        "                                     device='cuda',\n",
        "                                     pretrained=False)\n",
        "\n",
        "fx_parsing_random.extract(dataset_path=stimuli_path, \n",
        "                          save_format='npz', \n",
        "                          save_path='Parsing_Feats_random')\n",
        "\n",
        "\n",
        "## RDM Creation ##\n",
        "\n",
        "\n",
        "\n",
        "creator = RDMCreator(feat_path=\"Classification_Feats_random\", save_path=\"Classification_RDMs_random\")\n",
        "creator.create_rdms() \n",
        "\n",
        "creator = RDMCreator(feat_path=\"Parsing_Feats_random\", save_path=\"Parsing_RDMs_random\")\n",
        "creator.create_rdms() \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Evaluation ##\n",
        "evaluation = RSA(\"/content/Classification_RDMs_random\", \n",
        "                 roi_path, \n",
        "                 model_name=\"Scene Classification (random)\")\n",
        "\n",
        "df_classification_rdm = evaluation.evaluate() # Evaluation - Returns a pandas dataframe\n",
        "\n",
        "\n",
        "\n",
        "evaluation = RSA(\"/content/Parsing_RDMs_random\", \n",
        "                 roi_path, \n",
        "                 model_name=\"Scene Parsing (random)\")\n",
        "\n",
        "df_parsing_rdm = evaluation.evaluate() # Evaluation - Returns a pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzV5pNAsN-kX"
      },
      "outputs": [],
      "source": [
        "# Plotting with significance\n",
        "plotter = Plotting([df_classification, df_parsing, df_classification_rdm, df_parsing_rdm])\n",
        "results_dataframe = plotter.plot(pairs=sig_pairs, metric=\"R\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PhXmDhmt6Gd8"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
