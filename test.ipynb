{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {'autoencoding': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'curvature': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'colorization': \"taskonomy_colorization.TaskonomyEncoder\",\n",
    "          'class_object': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'class_scene': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'denoising': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'euclidean': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'depth': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'edge_occlusion': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'edge_texture': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'egomotion': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'fixated_pose': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'inpainting': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'jigsaw': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'keypoints2d': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'keypoints3d': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'nonfixated_pose': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'normal': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'point_matching': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'reshading': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'room_layout': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'segment_unsup2d': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'segment_unsup25d': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'segment_semantic': \"visualpriors.taskonomy_network.TaskonomyEncoder\",\n",
    "          'vanishing_point': \"visualpriors.taskonomy_network.TaskonomyEncoder\"}\n",
    "\n",
    "MODEL_NODES = {'autoencoding': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'curvature': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'class_object': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'class_scene': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'colorization': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'denoising': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'euclidean': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'depth': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'edge_occlusion': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'edge_texture': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'egomotion': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'fixated_pose': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'inpainting': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'jigsaw': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'keypoints2d': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'keypoints3d': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'nonfixated_pose': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'normal': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'point_matching': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'reshading': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'room_layout': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'segment_unsup2d': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'segment_unsup25d': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'segment_semantic': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
    "               'vanishing_point': ['layer1', 'layer2', 'layer3', 'layer4']}\n",
    "\n",
    "\n",
    "MODEL_WEIGHTS = {'autoencoding': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/autoencoding_encoder-e35146c09253720e97c0a7f8ee4e896ac931f5faa1449df003d81e6089ac6307.pth',\n",
    "                 'colorization': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/colorization_encoder-5ed817acdd28d13e443d98ad15ebe1c3059a3252396a2dff6a2090f6f86616a5.pth',\n",
    "                 'curvature': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/curvature_encoder-3767cf5d06d9c6bca859631eb5a3c368d66abeb15542171b94188ffbe47d7571.pth',\n",
    "                 'class_object': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/class_object_encoder-4a4e42dad58066039a0d2f9d128bb32e93a7e4aa52edb2d2a07bcdd1a6536c18.pth',\n",
    "                 'class_scene': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/class_scene_encoder-ad85764467cddafd98211313ceddebb98adf2a6bee2cedfe0b922a37ae65eaf8.pth',\n",
    "                 'denoising': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/denoising_encoder-b64cab95af4a2c565066a7e8effaf37d6586c3b9389b47fff9376478d849db38.pth',\n",
    "                 'euclidean': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/depth_euclidean_encoder-88f18d41313de7dbc88314a7f0feec3023047303d94d73eb8622dc40334ef149.pth',\n",
    "                 'depth': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/depth_zbuffer_encoder-cc343a8ed622fd7ee3ce54398be8682bbbbfb5d11fa80e8d03a56a5ae4e11b09.pth',\n",
    "                 'edge_occlusion': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/edge_occlusion_encoder-5ac3f3e918131f61e01fe95e49f462ae2fc56aa463f8d353ca84cd4e248b9c08.pth',\n",
    "                 'edge_texture': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/edge_texture_encoder-be2d686a6a4dfebe968d16146a17176eba37e29f736d5cd9a714317c93718810.pth',\n",
    "                 'egomotion': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/egomotion_encoder-9aa647c34bf98f9e491e0b37890d77566f6ae35ccf41d9375c674511318d571c.pth',\n",
    "                 'fixated_pose': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/fixated_pose_encoder-78cf321518abc16f9f4782b9e5d4e8f5d6966c373d951928a26f872e55297567.pth',\n",
    "                 'inpainting': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/inpainting_encoder-bf96fbaaea9268a820a19a1d13dbf6af31798f8983c6d9203c00fab2d236a142.pth',\n",
    "                 'jigsaw': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/jigsaw_encoder-0c2b342c9080f8713c178b04aa6c581ed3a0112fecaf78edc4c04e0a90516e39.pth',\n",
    "                 'keypoints2d': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/keypoints2d_encoder-6b77695acff4c84091c484a7b128a1e28a7e9c36243eda278598f582cf667fe0.pth',\n",
    "                 'keypoints3d': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/keypoints3d_encoder-7e3f1ec97b82ae30030b7ea4fec2dc606b71497d8c0335d05f0be3dc909d000d.pth',\n",
    "                 'nonfixated_pose': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/nonfixated_pose_encoder-3433a600ca9ff384b9898e55d86a186d572c2ebbe4701489a373933e3cfd5b8b.pth',\n",
    "                 'normal': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/normal_encoder-f5e2c7737e4948e3b2a822f584892c342eaabbe66661576ba50db7cdd40561c5.pth',\n",
    "                 'point_matching': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/point_matching_encoder-4bd2a6b2909d9998fabaf0278ab568f42f2b692a648e28555ede6c6cda5361f4.pth',\n",
    "                 'reshading': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/reshading_encoder-de456246e171dc8407fb2951539aa60d75925ae0f1dbb43f110b7768398b36a6.pth',\n",
    "                 'room_layout': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/room_layout_encoder-1e1662f43b834261464b1825227a04efba59b50cc8883bee9adc3ddafd4796c1.pth',\n",
    "                 'segment_unsup2d': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/segment_unsup2d_encoder-b679053a920e8bcabf0cd454606098ae85341e054080f2be29473971d4265964.pth',\n",
    "                 'segment_unsup25d': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/segment_unsup25d_encoder-7d12d2500c18c003ffc23943214f5dfd74932f0e3d03dde2c3a81ebc406e31a0.pth',\n",
    "                 'segment_semantic': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/segment_semantic_encoder-bb3007244520fc89cd111e099744a22b1e5c98cd83ed3f116fbff6376d220127.pth',\n",
    "                 'vanishing_point': 'https://github.com/alexsax/visual-prior/raw/networks/assets/pytorch/vanishing_point_encoder-afd2ae9b71d46a54efc5231b3e38ebc3e35bfab78cb0a78d9b75863a240b19a8.pth'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dicts_to_json(models, model_nodes, weights):\n",
    "    merged_dict = {}\n",
    "    for model_name, model_func in models.items():\n",
    "        merged_dict[model_name] = {\n",
    "            \"model\": str(model_func),\n",
    "            \"nodes\": model_nodes.get(model_name, []),\n",
    "            \"weights\": weights.get(model_name, [])\n",
    "        }\n",
    "    return json.dumps(merged_dict, indent=4)\n",
    "\n",
    "# Convert the MODELS and MODEL_NODES to JSON format\n",
    "json_output = dicts_to_json(MODELS, MODEL_NODES, MODEL_WEIGHTS)\n",
    "\n",
    "# If you want to save it to a file:\n",
    "with open(\"models_config.json\", \"w\") as file:\n",
    "    file.write(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out torch feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = resnet50()\n",
    "train_nodes, eval_nodes = get_graph_node_names(resnet50())\n",
    "\n",
    "pprint(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = create_feature_extractor(m, return_nodes=['layer4.0.bn1','layer4.0.relu'])#\n",
    "out = extractor(torch.zeros(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try my new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import FeatureExtractor\n",
    "\n",
    "path=\"C:/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/bonner_pnas2017/stimuli_data\"\n",
    "\n",
    "extractor = FeatureExtractor(\"AlexNet\", \"Standard\", path, device=\"cpu\")\n",
    "\n",
    "layers_to_extract = extractor.layers_to_extract()\n",
    "\n",
    "print(layers_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.extract([\"features.0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.consolidate_per_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path =\"features.3.npz\"\n",
    "\n",
    "data = np.load(path, allow_pickle=True)\n",
    "print(data.files)\n",
    "\n",
    "data['pathways0242.jpg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with Timm Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import FeatureExtractor\n",
    "\n",
    "path=\"C:/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/bonner_pnas2017/stimuli_data\"\n",
    "\n",
    "extractor = FeatureExtractor(\"resnet18\", \"Timm\", path, device=\"cpu\")\n",
    "\n",
    "extractor.extract(layers_to_extract=[\"conv1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.consolidate_per_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with Taskonomy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extraction import FeatureExtractor\n",
    "\n",
    "path=\"C:/Users/Domenic/Documents/Repositories/Net2Brain/notebooks/Workshops/bonner_pnas2017/stimuli_data\"\n",
    "\n",
    "extractor = FeatureExtractor(\"autoencoding\", \"Taskonomy\", path, device=\"cpu\")\n",
    "\n",
    "layers_to_extract = extractor.layers_to_extract()\n",
    "\n",
    "print(layers_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.extract(layers_to_extract=[\"conv1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "extractor.consolidate_per_layer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "N2B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
